{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Advanced LoRA Techniques\n",
    "\n",
    "**Portfolio Project: Building LLMs from Scratch on AWS** üöÄ\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/08_Advanced_LoRA.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Chapter Overview\n",
    "\n",
    "Advanced LoRA techniques for production deployment:\n",
    "- **Multiple Task-Specific Adapters**: Train different adapters for different tasks\n",
    "- **QLoRA**: Quantization + LoRA for extreme efficiency\n",
    "- **Adapter Switching**: Dynamic adapter selection\n",
    "- **A/B Testing**: Compare adapter performance\n",
    "- **Adapter Merging**: Combine multiple adapters\n",
    "- **Production Deployment**: Multi-adapter endpoints on AWS\n",
    "\n",
    "**Learning Objectives:**\n",
    "‚úÖ Multi-task learning with LoRA  \n",
    "‚úÖ 4-bit quantization with QLoRA  \n",
    "‚úÖ Dynamic adapter management  \n",
    "‚úÖ Production-ready A/B testing  \n",
    "\n",
    "**AWS Services:** SageMaker Multi-Model Endpoints, S3  \n",
    "**Estimated Cost:** $5-15\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup\n",
    "\n",
    "### Cell Purpose: Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch tiktoken matplotlib tqdm bitsandbytes\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "\n",
    "print(\"‚úÖ Environment ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Multi-Task LoRA Adapters\n",
    "\n",
    "### Cell Purpose: Train different adapters for different tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task datasets\n",
    "tasks = {\n",
    "    \"summarization\": [\n",
    "        (\"Summarize: Machine learning is a subset of AI.\", \"ML is part of AI technology.\"),\n",
    "        (\"Summarize: The weather today is sunny and warm.\", \"Today: sunny, warm weather.\"),\n",
    "        (\"Summarize: Python is a popular programming language.\", \"Python: popular language.\"),\n",
    "    ] * 10,\n",
    "    \n",
    "    \"translation\": [\n",
    "        (\"Translate to Spanish: Hello\", \"Hola\"),\n",
    "        (\"Translate to Spanish: Thank you\", \"Gracias\"),\n",
    "        (\"Translate to Spanish: Good morning\", \"Buenos d√≠as\"),\n",
    "    ] * 10,\n",
    "    \n",
    "    \"qa\": [\n",
    "        (\"Question: What is AI? Answer:\", \"AI is Artificial Intelligence.\"),\n",
    "        (\"Question: What is Python? Answer:\", \"Python is a programming language.\"),\n",
    "        (\"Question: What is ML? Answer:\", \"ML is Machine Learning.\"),\n",
    "    ] * 10,\n",
    "}\n",
    "\n",
    "class AdapterManager:\n",
    "    \"\"\"Manages multiple LoRA adapters for different tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "        self.adapters = {}  # task_name -> adapter_params\n",
    "        self.current_task = None\n",
    "        \n",
    "    def save_adapter(self, task_name, lora_params):\n",
    "        \"\"\"Save adapter parameters for a task\"\"\"\n",
    "        self.adapters[task_name] = {\n",
    "            name: param.data.clone()\n",
    "            for name, param in lora_params\n",
    "        }\n",
    "        print(f\"‚úÖ Saved adapter for task: {task_name}\")\n",
    "        \n",
    "    def load_adapter(self, task_name):\n",
    "        \"\"\"Load adapter parameters for a task\"\"\"\n",
    "        if task_name not in self.adapters:\n",
    "            raise ValueError(f\"No adapter found for task: {task_name}\")\n",
    "        \n",
    "        # Load adapter weights\n",
    "        for name, param in self.adapters[task_name].items():\n",
    "            # Find and update the parameter in model\n",
    "            for model_name, model_param in self.base_model.named_parameters():\n",
    "                if name in model_name and 'lora' in model_name:\n",
    "                    model_param.data = param.clone()\n",
    "        \n",
    "        self.current_task = task_name\n",
    "        print(f\"‚úÖ Loaded adapter for task: {task_name}\")\n",
    "        \n",
    "    def list_adapters(self):\n",
    "        \"\"\"List all available adapters\"\"\"\n",
    "        return list(self.adapters.keys())\n",
    "\n",
    "print(\"‚úÖ Multi-task adapter system ready!\")\n",
    "print(f\"   Tasks: {list(tasks.keys())}\")\n",
    "print(f\"   Samples per task: {[len(data) for data in tasks.values()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 QLoRA: Quantized LoRA\n",
    "\n",
    "### Cell Purpose: Implement 4-bit quantization with LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantize4bit:\n",
    "    \"\"\"Simple 4-bit quantization (educational implementation)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize(tensor):\n",
    "        \"\"\"Quantize tensor to 4-bit representation\"\"\"\n",
    "        # Find min and max\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        \n",
    "        # Scale to 0-15 (4-bit range)\n",
    "        scale = (max_val - min_val) / 15.0\n",
    "        quantized = torch.round((tensor - min_val) / scale).to(torch.uint8)\n",
    "        \n",
    "        return quantized, scale, min_val\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize(quantized, scale, min_val):\n",
    "        \"\"\"Dequantize from 4-bit back to float\"\"\"\n",
    "        return quantized.float() * scale + min_val\n",
    "\n",
    "class QLoRALayer(nn.Module):\n",
    "    \"\"\"QLoRA: Quantized base weights + LoRA adapters\"\"\"\n",
    "    \n",
    "    def __init__(self, linear_layer, rank=4, alpha=16, quantize=True):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Quantize base weights\n",
    "        if quantize:\n",
    "            quantized, scale, min_val = Quantize4bit.quantize(linear_layer.weight.data)\n",
    "            self.register_buffer('weight_quantized', quantized)\n",
    "            self.register_buffer('weight_scale', torch.tensor(scale))\n",
    "            self.register_buffer('weight_min', torch.tensor(min_val))\n",
    "            self.quantized = True\n",
    "        else:\n",
    "            self.weight = linear_layer.weight\n",
    "            self.quantized = False\n",
    "        \n",
    "        # Freeze base weights\n",
    "        if linear_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(linear_layer.bias.data.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        # LoRA adapters (trainable)\n",
    "        in_features = linear_layer.in_features\n",
    "        out_features = linear_layer.out_features\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get base weight\n",
    "        if self.quantized:\n",
    "            weight = Quantize4bit.dequantize(\n",
    "                self.weight_quantized,\n",
    "                self.weight_scale,\n",
    "                self.weight_min\n",
    "            )\n",
    "        else:\n",
    "            weight = self.weight\n",
    "        \n",
    "        # Base forward\n",
    "        output = F.linear(x, weight, self.bias)\n",
    "        \n",
    "        # Add LoRA\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        \n",
    "        return output + lora_output\n",
    "\n",
    "# Memory comparison\n",
    "print(\"=\"*60)\n",
    "print(\"MEMORY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Example: 1000x1000 weight matrix\n",
    "sample_weight = torch.randn(1000, 1000)\n",
    "\n",
    "# Full precision (FP32)\n",
    "fp32_size = sample_weight.element_size() * sample_weight.nelement()\n",
    "\n",
    "# 4-bit quantized\n",
    "quantized, _, _ = Quantize4bit.quantize(sample_weight)\n",
    "quant_size = quantized.element_size() * quantized.nelement()\n",
    "\n",
    "# LoRA adapters (rank=8)\n",
    "rank = 8\n",
    "lora_A_size = 1000 * rank * 4  # FP32\n",
    "lora_B_size = rank * 1000 * 4  # FP32\n",
    "\n",
    "print(f\"Original (FP32): {fp32_size / 1024:.2f} KB\")\n",
    "print(f\"Quantized (4-bit): {quant_size / 1024:.2f} KB\")\n",
    "print(f\"LoRA Adapters (rank={rank}): {(lora_A_size + lora_B_size) / 1024:.2f} KB\")\n",
    "print(f\"QLoRA Total: {(quant_size + lora_A_size + lora_B_size) / 1024:.2f} KB\")\n",
    "print(f\"\\nMemory Reduction: {(1 - (quant_size + lora_A_size + lora_B_size) / fp32_size) * 100:.1f}%\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTestingFramework:\n",
    "    \"\"\"A/B testing for comparing adapter performance\"\"\"\n",
    "    \n",
    "    def __init__(self, model, adapter_manager):\n",
    "        self.model = model\n",
    "        self.adapter_manager = adapter_manager\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_test(self, test_data, adapters_to_test, metric_fn):\n",
    "        \"\"\"\n",
    "        Run A/B test on multiple adapters\n",
    "        \n",
    "        Args:\n",
    "            test_data: List of (input, expected_output) tuples\n",
    "            adapters_to_test: List of adapter names\n",
    "            metric_fn: Function to compute metric (higher is better)\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"A/B TESTING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for adapter_name in adapters_to_test:\n",
    "            print(f\"\\nTesting adapter: {adapter_name}\")\n",
    "            \n",
    "            # Load adapter\n",
    "            self.adapter_manager.load_adapter(adapter_name)\n",
    "            \n",
    "            # Evaluate\n",
    "            scores = []\n",
    "            for input_text, expected in test_data:\n",
    "                # Generate output (simplified)\n",
    "                with torch.no_grad():\n",
    "                    score = metric_fn(input_text, expected, self.model)\n",
    "                    scores.append(score)\n",
    "            \n",
    "            # Store results\n",
    "            avg_score = np.mean(scores)\n",
    "            self.results[adapter_name] = {\n",
    "                'scores': scores,\n",
    "                'mean': avg_score,\n",
    "                'std': np.std(scores),\n",
    "                'n_samples': len(scores)\n",
    "            }\n",
    "            \n",
    "            print(f\"   Mean Score: {avg_score:.4f} ¬± {np.std(scores):.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        self._print_summary()\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print comparison summary\"\"\"\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Sort by mean score\n",
    "        sorted_adapters = sorted(\n",
    "            self.results.items(),\n",
    "            key=lambda x: x[1]['mean'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{'Rank':<6}{'Adapter':<20}{'Score':<15}{'Samples'}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for rank, (adapter_name, results) in enumerate(sorted_adapters, 1):\n",
    "            print(f\"{rank:<6}{adapter_name:<20}\"\n",
    "                  f\"{results['mean']:.4f} ¬± {results['std']:.4f}  \"\n",
    "                  f\"{results['n_samples']}\")\n",
    "        \n",
    "        # Winner\n",
    "        winner = sorted_adapters[0][0]\n",
    "        print(f\"\\nüèÜ Winner: {winner}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    def get_winner(self):\n",
    "        \"\"\"Get best performing adapter\"\"\"\n",
    "        return max(self.results.items(), key=lambda x: x[1]['mean'])[0]\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize A/B test results\"\"\"\n",
    "        adapters = list(self.results.keys())\n",
    "        means = [self.results[a]['mean'] for a in adapters]\n",
    "        stds = [self.results[a]['std'] for a in adapters]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x_pos = np.arange(len(adapters))\n",
    "        plt.bar(x_pos, means, yerr=stds, capsize=5, \n",
    "                color=['#3498db', '#2ecc71', '#e74c3c'][:len(adapters)],\n",
    "                edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Adapter', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Score', fontsize=12, fontweight='bold')\n",
    "        plt.title('A/B Test Results', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(x_pos, adapters, rotation=45)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example metric function\n",
    "def simple_match_metric(input_text, expected, model):\n",
    "    \"\"\"Simple metric: 1 if length matches, 0 otherwise\"\"\"\n",
    "    return 1.0 if len(input_text) > 0 else 0.0\n",
    "\n",
    "print(\"‚úÖ A/B Testing framework ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterMerger:\n",
    "    \"\"\"Merge multiple LoRA adapters\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_merge(adapters, weights=None):\n",
    "        \"\"\"\n",
    "        Merge adapters by averaging\n",
    "        \n",
    "        Args:\n",
    "            adapters: List of adapter parameter dicts\n",
    "            weights: Optional weights for weighted average\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(adapters)] * len(adapters)\n",
    "        \n",
    "        merged = {}\n",
    "        \n",
    "        # Get all parameter names from first adapter\n",
    "        param_names = list(adapters[0].keys())\n",
    "        \n",
    "        for name in param_names:\n",
    "            # Weighted average of parameters\n",
    "            merged[name] = sum(\n",
    "                w * adapter[name] for w, adapter in zip(weights, adapters)\n",
    "            )\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    @staticmethod\n",
    "    def task_vector_merge(adapter_base, adapter_A, adapter_B, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Merge using task vectors\n",
    "        \n",
    "        Task vector = adapter_params - base_params\n",
    "        Merged = base + alpha * (taskA + taskB)\n",
    "        \"\"\"\n",
    "        merged = {}\n",
    "        \n",
    "        for name in adapter_base.keys():\n",
    "            # Compute task vectors\n",
    "            task_vector_A = adapter_A[name] - adapter_base[name]\n",
    "            task_vector_B = adapter_B[name] - adapter_base[name]\n",
    "            \n",
    "            # Merge\n",
    "            merged[name] = adapter_base[name] + alpha * (task_vector_A + task_vector_B)\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_adapter_similarity(adapters, names):\n",
    "        \"\"\"Visualize similarity between adapters\"\"\"\n",
    "        n_adapters = len(adapters)\n",
    "        similarity_matrix = np.zeros((n_adapters, n_adapters))\n",
    "        \n",
    "        for i in range(n_adapters):\n",
    "            for j in range(n_adapters):\n",
    "                # Compute cosine similarity of flattened parameters\n",
    "                params_i = torch.cat([p.flatten() for p in adapters[i].values()])\n",
    "                params_j = torch.cat([p.flatten() for p in adapters[j].values()])\n",
    "                \n",
    "                similarity = F.cosine_similarity(\n",
    "                    params_i.unsqueeze(0),\n",
    "                    params_j.unsqueeze(0)\n",
    "                ).item()\n",
    "                \n",
    "                similarity_matrix[i, j] = similarity\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(similarity_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "        plt.colorbar(label='Cosine Similarity')\n",
    "        plt.xticks(range(n_adapters), names, rotation=45)\n",
    "        plt.yticks(range(n_adapters), names)\n",
    "        plt.title('Adapter Similarity Matrix', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(n_adapters):\n",
    "            for j in range(n_adapters):\n",
    "                plt.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                        ha='center', va='center',\n",
    "                        color='black' if similarity_matrix[i, j] > 0.5 else 'white')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ Adapter merging tools ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Production Deployment\n",
    "\n",
    "### Cell Purpose: AWS multi-model endpoint setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_guide = \"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "AWS MULTI-ADAPTER DEPLOYMENT\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Base Model (Frozen) + Multiple LoRA Adapters\n",
    "‚îî‚îÄ‚îÄ Adapter Registry (S3)\n",
    "    ‚îú‚îÄ‚îÄ summarization_adapter.pth\n",
    "    ‚îú‚îÄ‚îÄ translation_adapter.pth\n",
    "    ‚îî‚îÄ‚îÄ qa_adapter.pth\n",
    "\n",
    "## Deployment Strategy\n",
    "\n",
    "### Option 1: Multi-Model Endpoint\n",
    "--------------------------------------------------------------\n",
    "- Single endpoint serving multiple adapters\n",
    "- Base model loaded once in memory\n",
    "- Dynamically load adapters per request\n",
    "- Most cost-effective for multiple tasks\n",
    "\n",
    "```python\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "# Create multi-model\n",
    "mdm = MultiDataModel(\n",
    "    name='multi-adapter-endpoint',\n",
    "    model_data_prefix='s3://bucket/adapters/',\n",
    "    model=base_model,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy\n",
    "predictor = mdm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge'\n",
    ")\n",
    "\n",
    "# Invoke with specific adapter\n",
    "response = predictor.predict(\n",
    "    data={'text': 'Hello'},\n",
    "    target_model='summarization_adapter.tar.gz'\n",
    ")\n",
    "```\n",
    "\n",
    "### Option 2: Adapter-per-Endpoint\n",
    "--------------------------------------------------------------\n",
    "- Separate endpoint for each task\n",
    "- Best for high-traffic scenarios\n",
    "- Auto-scaling per task\n",
    "\n",
    "### Option 3: Serverless with Lambda\n",
    "--------------------------------------------------------------\n",
    "- Best for low/sporadic traffic\n",
    "- Cold start: ~2-5 seconds\n",
    "- Cost: Pay per request\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### 1. Prepare Adapters\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "# Save each adapter separately\n",
    "for task_name, adapter in adapters.items():\n",
    "    torch.save({\n",
    "        'adapter_params': adapter,\n",
    "        'rank': 8,\n",
    "        'alpha': 16,\n",
    "        'task': task_name\n",
    "    }, f'{task_name}_adapter.pth')\n",
    "    \n",
    "    # Upload to S3\n",
    "    !aws s3 cp {task_name}_adapter.pth s3://bucket/adapters/\n",
    "```\n",
    "\n",
    "### 2. Create Inference Handler\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "# inference.py\n",
    "class ModelHandler:\n",
    "    def __init__(self):\n",
    "        self.base_model = load_base_model()\n",
    "        self.current_adapter = None\n",
    "        \n",
    "    def load_adapter(self, adapter_name):\n",
    "        if adapter_name != self.current_adapter:\n",
    "            adapter_path = f'/opt/ml/model/{adapter_name}'\n",
    "            load_lora_adapter(self.base_model, adapter_path)\n",
    "            self.current_adapter = adapter_name\n",
    "    \n",
    "    def predict(self, data):\n",
    "        adapter = data.get('adapter', 'default')\n",
    "        self.load_adapter(adapter)\n",
    "        return self.base_model.generate(data['text'])\n",
    "```\n",
    "\n",
    "### 3. Deploy Multi-Model Endpoint\n",
    "--------------------------------------------------------------\n",
    "```bash\n",
    "# Package base model\n",
    "tar -czf base_model.tar.gz model/ code/\n",
    "\n",
    "# Upload\n",
    "aws s3 cp base_model.tar.gz s3://bucket/models/\n",
    "\n",
    "# Create SageMaker model\n",
    "aws sagemaker create-model \\\\\n",
    "    --model-name multi-adapter-model \\\\\n",
    "    --primary-container \\\\\n",
    "        Image=pytorch-inference:2.0 \\\\\n",
    "        ModelDataUrl=s3://bucket/models/base_model.tar.gz \\\\\n",
    "        Mode=MultiModel\n",
    "\n",
    "# Create endpoint\n",
    "aws sagemaker create-endpoint \\\\\n",
    "    --endpoint-name multi-adapter-endpoint \\\\\n",
    "    --endpoint-config-name multi-adapter-config\n",
    "```\n",
    "\n",
    "### 4. A/B Testing in Production\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "# Route traffic to different adapters\n",
    "from sagemaker import ProductionVariant\n",
    "\n",
    "variants = [\n",
    "    ProductionVariant(\n",
    "        variant_name='AdapterV1',\n",
    "        model_name='adapter_v1',\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.g4dn.xlarge',\n",
    "        initial_variant_weight=70  # 70% traffic\n",
    "    ),\n",
    "    ProductionVariant(\n",
    "        variant_name='AdapterV2',\n",
    "        model_name='adapter_v2',\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.g4dn.xlarge',\n",
    "        initial_variant_weight=30  # 30% traffic\n",
    "    )\n",
    "]\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name='ab-test-endpoint',\n",
    "    production_variants=variants\n",
    ")\n",
    "```\n",
    "\n",
    "## Cost Analysis\n",
    "\n",
    "### Multi-Model Endpoint (Recommended)\n",
    "--------------------------------------------------------------\n",
    "Instance: ml.g4dn.xlarge (1x T4 GPU)\n",
    "Base Cost: ~$0.736/hour\n",
    "\n",
    "Storage (S3):\n",
    "- Base model (500 MB): $0.01/month\n",
    "- 10 adapters (5 MB each): $0.001/month\n",
    "- Total: ~$0.011/month\n",
    "\n",
    "Monthly Cost (24/7):\n",
    "- Compute: ~$530/month\n",
    "- Storage: ~$0.011/month\n",
    "- Total: ~$530/month\n",
    "\n",
    "With Auto-Scaling (typical):\n",
    "- Min instances: 0 (after hours)\n",
    "- Max instances: 3 (peak hours)\n",
    "- Average: ~$150-300/month\n",
    "\n",
    "### Serverless (Low Traffic)\n",
    "--------------------------------------------------------------\n",
    "Cost per invocation: ~$0.0001\n",
    "10,000 requests/month: ~$1\n",
    "100,000 requests/month: ~$10\n",
    "\n",
    "Best for: <100k requests/month\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "```python\n",
    "# CloudWatch metrics\n",
    "import boto3\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Track adapter usage\n",
    "cloudwatch.put_metric_data(\n",
    "    Namespace='MultiAdapter',\n",
    "    MetricData=[{\n",
    "        'MetricName': 'AdapterInvocations',\n",
    "        'Dimensions': [\n",
    "            {'Name': 'AdapterName', 'Value': adapter_name}\n",
    "        ],\n",
    "        'Value': 1.0\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Track latency per adapter\n",
    "# Track error rates\n",
    "# Track model drift\n",
    "```\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Chapter Summary\n",
    "\n",
    "### What We Built:\n",
    "1. ‚úÖ **Multi-Task Adapters**: Train different adapters for different tasks\n",
    "2. ‚úÖ **QLoRA**: 4-bit quantization for 75% memory reduction\n",
    "3. ‚úÖ **A/B Testing**: Compare adapter performance systematically\n",
    "4. ‚úÖ **Adapter Merging**: Combine multiple adapters intelligently\n",
    "5. ‚úÖ **Production Deployment**: Multi-model endpoints on AWS\n",
    "\n",
    "### Key Concepts:\n",
    "- **Task-Specific Adapters**: One base model, many specialized adapters\n",
    "- **Quantization**: Reduce model size with minimal accuracy loss\n",
    "- **A/B Testing**: Data-driven adapter selection\n",
    "- **Adapter Merging**: Combine capabilities from multiple adapters\n",
    "- **Multi-Model Endpoints**: Efficient serving of multiple models\n",
    "\n",
    "### QLoRA Benefits:\n",
    "- **Memory**: 75-80% reduction vs LoRA\n",
    "- **Speed**: Similar inference speed\n",
    "- **Quality**: <1% accuracy loss\n",
    "- **Cost**: Train larger models on smaller GPUs\n",
    "\n",
    "### Production Best Practices:\n",
    "1. **Adapter Registry**: Central storage for all adapters (S3)\n",
    "2. **Version Control**: Track adapter versions\n",
    "3. **Monitoring**: Per-adapter metrics\n",
    "4. **Caching**: Keep frequently-used adapters in memory\n",
    "5. **Fallback**: Default adapter for unknown tasks\n",
    "\n",
    "### Cost Comparison:\n",
    "| Deployment Type | Monthly Cost | Best For |\n",
    "|----------------|--------------|----------|\n",
    "| Multi-Model (24/7) | ~$530 | High traffic |\n",
    "| Multi-Model (Auto-scale) | ~$150-300 | Variable traffic |\n",
    "| Serverless | ~$0.10/1k requests | Low/sporadic |\n",
    "| Per-Adapter Endpoints | ~$530 per task | Critical tasks |\n",
    "\n",
    "### Next Steps:\n",
    "‚û°Ô∏è **Chapter 9**: Evaluation metrics (ROUGE, BLEU, Perplexity)  \n",
    "‚û°Ô∏è **Advanced**: Mixture of Experts (MoE)  \n",
    "‚û°Ô∏è **Research**: Adapter fusion techniques  \n",
    "\n",
    "---\n",
    "\n",
    "## üîó Resources\n",
    "\n",
    "**Papers:**\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [AdapterHub](https://arxiv.org/abs/2007.07779)\n",
    "- [Task Arithmetic](https://arxiv.org/abs/2212.04089)\n",
    "\n",
    "**AWS Documentation:**\n",
    "- [Multi-Model Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html)\n",
    "- [SageMaker A/B Testing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html)\n",
    "\n",
    "**Tools:**\n",
    "- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) - Quantization\n",
    "- [PEFT](https://github.com/huggingface/peft) - Parameter-efficient fine-tuning\n",
    "- [AdapterHub](https://adapterhub.ml/) - Adapter repository\n",
    "\n",
    "**Ready for evaluation metrics? üìä**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
