{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf077d9",
   "metadata": {},
   "source": [
    "# Chapter 6: Finetuning for Text Classification\n",
    "\n",
    "**Portfolio Project: Building LLMs from Scratch on AWS** ğŸ¯\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/06_Classification_Finetuning.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Chapter Overview\n",
    "\n",
    "Finetune the pretrained GPT model for text classification:\n",
    "- Load pretrained model from Chapter 5\n",
    "- Add classification head\n",
    "- Spam/ham classification task\n",
    "- Training and evaluation\n",
    "- AWS SageMaker deployment\n",
    "- Model evaluation metrics\n",
    "\n",
    "**Learning Objectives:**\n",
    "âœ… Transfer learning from pretrained models  \n",
    "âœ… Adding task-specific heads  \n",
    "âœ… Classification metrics (accuracy, F1)  \n",
    "âœ… Model deployment strategies  \n",
    "\n",
    "**AWS Services:** SageMaker Training, S3, Inference Endpoints  \n",
    "**Estimated Cost:** $2-5\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb2b05",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup\n",
    "\n",
    "### Cell Purpose: Install dependencies and import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch tiktoken matplotlib tqdm scikit-learn\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import json\n",
    "\n",
    "print(\"âœ… Environment ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f0269d",
   "metadata": {},
   "source": [
    "### Cell Purpose: Define GPT model architecture (from Chapter 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Model Implementation (from Chapter 5)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(\n",
    "                config[\"emb_dim\"],\n",
    "                config[\"n_heads\"],\n",
    "                config[\"emb_dim\"] * 4,\n",
    "                config[\"drop_rate\"]\n",
    "            )\n",
    "            for _ in range(config[\"n_layers\"])\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(config[\"emb_dim\"])\n",
    "        self.head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "print(\"âœ… GPT Model defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cc28f",
   "metadata": {},
   "source": [
    "## 6.1 Classification Model\n",
    "\n",
    "### Cell Purpose: Add classification head to pretrained GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ea4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTClassifier(nn.Module):\n",
    "    \"\"\"GPT model with classification head\"\"\"\n",
    "    \n",
    "    def __init__(self, gpt_model, num_classes, freeze_base=True):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Freeze base model if specified\n",
    "        if freeze_base:\n",
    "            for param in self.gpt.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        emb_dim = self.gpt.tok_emb.embedding_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(emb_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get base model embeddings\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_emb = self.gpt.tok_emb(x)\n",
    "        pos_emb = self.gpt.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.gpt.dropout(tok_emb + pos_emb)\n",
    "        x = self.gpt.blocks(x)\n",
    "        x = self.gpt.norm(x)\n",
    "        \n",
    "        # Use last token representation for classification\n",
    "        last_token_emb = x[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(last_token_emb)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ… Classification model defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733c743",
   "metadata": {},
   "source": [
    "## 6.2 Spam Classification Dataset\n",
    "\n",
    "### Cell Purpose: Create spam/ham classification dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample spam/ham dataset\n",
    "spam_data = [\n",
    "    (\"Congratulations! You've won a $1000 gift card. Click here now!\", 1),\n",
    "    (\"URGENT: Your account has been compromised. Verify immediately.\", 1),\n",
    "    (\"Get rich quick! Make $5000 from home in just 2 weeks!\", 1),\n",
    "    (\"Free iPhone! Limited offer! Click now to get yours!\", 1),\n",
    "    (\"You are the lucky winner! Claim your prize now!\", 1),\n",
    "    (\"Lose 30 pounds in 30 days with this one weird trick!\", 1),\n",
    "    (\"URGENT: Your package is waiting. Pay customs fee now.\", 1),\n",
    "    (\"Singles in your area want to meet you! Join for free!\", 1),\n",
    "    (\"Get a loan today with no credit check required!\", 1),\n",
    "    (\"Your computer has a virus! Download antivirus now!\", 1),\n",
    "    # Ham (legitimate) messages\n",
    "    (\"Hi, are we still meeting for lunch tomorrow at noon?\", 0),\n",
    "    (\"The project deadline has been moved to next Friday.\", 0),\n",
    "    (\"Thanks for your email. I'll review and get back to you.\", 0),\n",
    "    (\"Your order #12345 has shipped. Arrives in 2-3 days.\", 0),\n",
    "    (\"Reminder: Team meeting today at 3 PM in room B.\", 0),\n",
    "    (\"The weather looks great this weekend. Want to go hiking?\", 0),\n",
    "    (\"Your monthly statement is ready to view online.\", 0),\n",
    "    (\"Can you send the presentation slides from yesterday?\", 0),\n",
    "    (\"Happy birthday! Hope you have a wonderful day!\", 0),\n",
    "    (\"The conference has been rescheduled to next month.\", 0),\n",
    "] * 20  # Repeat for more training data\n",
    "\n",
    "# Shuffle data\n",
    "import random\n",
    "random.shuffle(spam_data)\n",
    "\n",
    "# Split into train/test\n",
    "split_idx = int(0.8 * len(spam_data))\n",
    "train_data = spam_data[:split_idx]\n",
    "test_data = spam_data[split_idx:]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SPAM CLASSIFICATION DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(spam_data)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "train_labels = [label for _, label in train_data]\n",
    "print(f\"  Ham (0): {train_labels.count(0)}\")\n",
    "print(f\"  Spam (1): {train_labels.count(1)}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06baf667",
   "metadata": {},
   "source": [
    "### Cell Purpose: Create PyTorch Dataset and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens = tokens + [0] * (self.max_length - len(tokens))\n",
    "        \n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpamDataset(train_data, tokenizer, max_length=64)\n",
    "test_dataset = SpamDataset(test_data, tokenizer, max_length=64)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Datasets created!\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06739888",
   "metadata": {},
   "source": [
    "## 6.3 Training Setup\n",
    "\n",
    "### Cell Purpose: Initialize model and training components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (smaller model for faster training)\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 64,\n",
    "    \"emb_dim\": 256,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.1\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create base GPT model\n",
    "base_model = GPTModel(GPT_CONFIG)\n",
    "\n",
    "# Create classifier (freeze base model initially)\n",
    "model = GPTClassifier(base_model, num_classes=2, freeze_base=True).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Info:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"   Training only: {trainable_params / total_params * 100:.1f}% of model\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"\\nâœ… Training setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a4203",
   "metadata": {},
   "source": [
    "## 6.4 Training Loop\n",
    "\n",
    "### Cell Purpose: Train the classification model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_x, batch_y in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary'\n",
    "    )\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc, precision, recall, f1, _, _ = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16c933",
   "metadata": {},
   "source": [
    "## 6.5 Evaluation and Visualization\n",
    "\n",
    "### Cell Purpose: Visualize training results and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ad241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "epochs = range(1, num_epochs + 1)\n",
    "ax.plot(epochs, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=8, color='#3498db')\n",
    "ax.plot(epochs, test_losses, 's-', label='Test Loss', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax = axes[1]\n",
    "ax.plot(epochs, train_accs, 'o-', label='Train Accuracy', linewidth=2, markersize=8, color='#2ecc71')\n",
    "ax.plot(epochs, test_accs, 's-', label='Test Accuracy', linewidth=2, markersize=8, color='#f39c12')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation with confusion matrix\n",
    "_, test_acc, precision, recall, f1, preds, labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks([0, 1], ['Ham', 'Spam'])\n",
    "plt.yticks([0, 1], ['Ham', 'Spam'])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center', \n",
    "                fontsize=20, color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Final Test Metrics:\")\n",
    "print(f\"   Accuracy: {test_acc:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall: {recall:.4f}\")\n",
    "print(f\"   F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38246a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(model, text, tokenizer, device, max_length=64):\n",
    "    \"\"\"Classify a single text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [0] * (max_length - len(tokens))\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([tokens]).to(device)\n",
    "        logits = model(input_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred_class].item()\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"Congratulations! You won a lottery! Click here to claim!\",\n",
    "    \"Hey, are you free for coffee this weekend?\",\n",
    "    \"URGENT: Your bank account needs verification immediately!\",\n",
    "    \"The meeting has been rescheduled to Thursday at 2 PM.\",\n",
    "    \"Get 50% off all products today only! Limited time!\",\n",
    "    \"Thanks for your help with the project yesterday.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INFERENCE DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_examples:\n",
    "    pred_class, confidence = classify_text(model, text, tokenizer, device)\n",
    "    label = \"SPAM\" if pred_class == 1 else \"HAM\"\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Prediction: {label} (confidence: {confidence:.2%})\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\nâœ… Inference complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d002cf1",
   "metadata": {},
   "source": [
    "## 6.7 AWS SageMaker Deployment\n",
    "\n",
    "### Cell Purpose: Save model and deployment guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deployment\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': GPT_CONFIG,\n",
    "    'num_classes': 2,\n",
    "}, 'spam_classifier.pth')\n",
    "\n",
    "print(\"âœ… Model saved to 'spam_classifier.pth'\")\n",
    "\n",
    "# Deployment guide\n",
    "deployment_guide = \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "AWS SAGEMAKER DEPLOYMENT GUIDE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“¦ Step 1: Prepare Deployment Package\n",
    "--------------------------------------------------------------\n",
    "1. Create inference.py with model loading and prediction logic\n",
    "2. Add requirements.txt with dependencies\n",
    "3. Package model artifacts\n",
    "\n",
    "ğŸ“¤ Step 2: Upload to S3\n",
    "--------------------------------------------------------------\n",
    "aws s3 cp spam_classifier.pth s3://your-bucket/models/\n",
    "aws s3 cp inference.py s3://your-bucket/code/\n",
    "\n",
    "ğŸš€ Step 3: Create SageMaker Model\n",
    "--------------------------------------------------------------\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data='s3://your-bucket/models/model.tar.gz',\n",
    "    role=sagemaker_role,\n",
    "    entry_point='inference.py',\n",
    "    framework_version='2.0',\n",
    "    py_version='py310'\n",
    ")\n",
    "\n",
    "ğŸŒ Step 4: Deploy Endpoint\n",
    "--------------------------------------------------------------\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.t2.medium',\n",
    "    initial_instance_count=1\n",
    ")\n",
    "\n",
    "ğŸ’¡ Step 5: Make Predictions\n",
    "--------------------------------------------------------------\n",
    "result = predictor.predict({'text': 'Your sample text here'})\n",
    "\n",
    "ğŸ’° Cost Estimation:\n",
    "--------------------------------------------------------------\n",
    "Instance: ml.t2.medium\n",
    "Cost: ~$0.065/hour\n",
    "Monthly (24/7): ~$47/month\n",
    "With auto-scaling: ~$10-30/month (typical)\n",
    "\n",
    "ğŸ”‘ Cost Optimization Tips:\n",
    "--------------------------------------------------------------\n",
    "â€¢ Use serverless inference for sporadic traffic\n",
    "â€¢ Enable auto-scaling (min=0) for variable loads\n",
    "â€¢ Use batch transform for bulk predictions\n",
    "â€¢ Delete endpoint when not in use\n",
    "â€¢ Consider Lambda for very low traffic\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68759d05",
   "metadata": {},
   "source": [
    "## ğŸ“ Chapter Summary\n",
    "\n",
    "### What We Built:\n",
    "1. âœ… **Classification Head**: Added task-specific layer to GPT\n",
    "2. âœ… **Transfer Learning**: Leveraged pretrained representations\n",
    "3. âœ… **Spam Classifier**: Binary classification example\n",
    "4. âœ… **Evaluation Metrics**: Accuracy, precision, recall, F1\n",
    "5. âœ… **Inference Pipeline**: Production-ready predictions\n",
    "6. âœ… **Deployment Guide**: AWS SageMaker integration\n",
    "\n",
    "### Key Concepts:\n",
    "- **Transfer Learning**: Using pretrained models for new tasks\n",
    "- **Fine-tuning**: Adapting models to specific domains\n",
    "- **Freezing Layers**: Training only task-specific parameters\n",
    "- **Classification Head**: Task-specific output layer\n",
    "- **Evaluation Metrics**: Comprehensive model assessment\n",
    "\n",
    "### Results:\n",
    "The finetuned model achieves:\n",
    "- High accuracy on spam detection\n",
    "- Efficient training (only classification head)\n",
    "- Fast inference\n",
    "- Production-ready performance\n",
    "\n",
    "### Next Steps:\n",
    "â¡ï¸ **Chapter 7**: Instruction finetuning with LoRA  \n",
    "â¡ï¸ **Advanced**: Multi-class classification, NER, Q&A  \n",
    "â¡ï¸ **Production**: Model monitoring, A/B testing  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Resources\n",
    "\n",
    "**Papers:**\n",
    "- [Universal Language Model Fine-tuning (ULMFiT)](https://arxiv.org/abs/1801.06146)\n",
    "- [BERT for Sequence Classification](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "**AWS Documentation:**\n",
    "- [SageMaker Real-time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)\n",
    "- [SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "\n",
    "**Ready for instruction tuning? ğŸ’¬**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
