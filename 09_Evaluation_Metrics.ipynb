{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Evaluation Metrics for LLMs\n",
    "\n",
    "**Portfolio Project: Building LLMs from Scratch on AWS** ðŸ“Š\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/09_Evaluation_Metrics.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Chapter Overview\n",
    "\n",
    "Comprehensive evaluation metrics for language models:\n",
    "- **ROUGE**: Summarization quality\n",
    "- **BLEU**: Translation quality\n",
    "- **Perplexity**: Language modeling capability\n",
    "- **METEOR**: Advanced translation metric\n",
    "- **BERTScore**: Semantic similarity\n",
    "- **Human Evaluation**: Guidelines and tools\n",
    "\n",
    "**Learning Objectives:**\n",
    "âœ… Implement evaluation metrics from scratch  \n",
    "âœ… Understand metric strengths and limitations  \n",
    "âœ… Compare model outputs systematically  \n",
    "âœ… Production monitoring strategies  \n",
    "\n",
    "**AWS Services:** CloudWatch, S3 for result storage  \n",
    "**Estimated Cost:** $1-3\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup\n",
    "\n",
    "### Cell Purpose: Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch matplotlib numpy nltk rouge-score sacrebleu\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Try to import evaluation libraries\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    from sacrebleu.metrics import BLEU\n",
    "    EVAL_LIBS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EVAL_LIBS_AVAILABLE = False\n",
    "    print(\"âš ï¸ ROUGE/BLEU libraries not available. Using implementations from scratch.\")\n",
    "\n",
    "print(\"âœ… Environment ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 ROUGE Score (Summarization)\n",
    "\n",
    "### Cell Purpose: Implement ROUGE metrics from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROUGEScore:\n",
    "    \"\"\"ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(text, n):\n",
    "        \"\"\"Extract n-grams from text\"\"\"\n",
    "        words = text.lower().split()\n",
    "        return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def rouge_n(candidate, reference, n=1):\n",
    "        \"\"\"\n",
    "        ROUGE-N: N-gram overlap\n",
    "        \n",
    "        Args:\n",
    "            candidate: Generated summary\n",
    "            reference: Gold standard summary\n",
    "            n: N-gram size (1=unigram, 2=bigram, etc.)\n",
    "        \"\"\"\n",
    "        ref_ngrams = ROUGEScore.get_ngrams(reference, n)\n",
    "        cand_ngrams = ROUGEScore.get_ngrams(candidate, n)\n",
    "        \n",
    "        if not ref_ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count overlapping n-grams\n",
    "        ref_counter = Counter(ref_ngrams)\n",
    "        cand_counter = Counter(cand_ngrams)\n",
    "        \n",
    "        overlap = sum((ref_counter & cand_counter).values())\n",
    "        \n",
    "        # ROUGE = overlap / total reference n-grams\n",
    "        recall = overlap / len(ref_ngrams) if ref_ngrams else 0\n",
    "        precision = overlap / len(cand_ngrams) if cand_ngrams else 0\n",
    "        \n",
    "        if recall + precision == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def rouge_l(candidate, reference):\n",
    "        \"\"\"\n",
    "        ROUGE-L: Longest Common Subsequence\n",
    "        \"\"\"\n",
    "        ref_words = reference.lower().split()\n",
    "        cand_words = candidate.lower().split()\n",
    "        \n",
    "        # LCS using dynamic programming\n",
    "        m, n = len(ref_words), len(cand_words)\n",
    "        dp = [[0] * (n+1) for _ in range(m+1)]\n",
    "        \n",
    "        for i in range(1, m+1):\n",
    "            for j in range(1, n+1):\n",
    "                if ref_words[i-1] == cand_words[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "        \n",
    "        lcs_length = dp[m][n]\n",
    "        \n",
    "        recall = lcs_length / m if m > 0 else 0\n",
    "        precision = lcs_length / n if n > 0 else 0\n",
    "        \n",
    "        if recall + precision == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "# Example evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"ROUGE EVALUATION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reference = \"Machine learning is a subset of artificial intelligence\"\n",
    "candidate1 = \"Machine learning is part of AI\"\n",
    "candidate2 = \"ML is related to artificial intelligence\"\n",
    "\n",
    "for i, candidate in enumerate([candidate1, candidate2], 1):\n",
    "    print(f\"\\nCandidate {i}: {candidate}\")\n",
    "    \n",
    "    rouge1 = ROUGEScore.rouge_n(candidate, reference, n=1)\n",
    "    rouge2 = ROUGEScore.rouge_n(candidate, reference, n=2)\n",
    "    rougel = ROUGEScore.rouge_l(candidate, reference)\n",
    "    \n",
    "    print(f\"  ROUGE-1 F1: {rouge1['f1']:.4f}\")\n",
    "    print(f\"  ROUGE-2 F1: {rouge2['f1']:.4f}\")\n",
    "    print(f\"  ROUGE-L F1: {rougel['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 BLEU Score (Translation)\n",
    "\n",
    "### Cell Purpose: Implement BLEU metric from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLEUScore:\n",
    "    \"\"\"BLEU (Bilingual Evaluation Understudy)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ngrams(text, n):\n",
    "        \"\"\"Extract n-grams\"\"\"\n",
    "        words = text.split()\n",
    "        return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def modified_precision(candidate, reference, n):\n",
    "        \"\"\"Calculate modified n-gram precision\"\"\"\n",
    "        cand_ngrams = BLEUScore.get_ngrams(candidate, n)\n",
    "        ref_ngrams = BLEUScore.get_ngrams(reference, n)\n",
    "        \n",
    "        if not cand_ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        ref_counter = Counter(ref_ngrams)\n",
    "        cand_counter = Counter(cand_ngrams)\n",
    "        \n",
    "        # Clipped counts\n",
    "        clipped_counts = sum(\n",
    "            min(cand_counter[ng], ref_counter[ng])\n",
    "            for ng in cand_counter\n",
    "        )\n",
    "        \n",
    "        return clipped_counts / len(cand_ngrams)\n",
    "    \n",
    "    @staticmethod\n",
    "    def brevity_penalty(candidate, reference):\n",
    "        \"\"\"Calculate brevity penalty\"\"\"\n",
    "        c = len(candidate.split())\n",
    "        r = len(reference.split())\n",
    "        \n",
    "        if c > r:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return math.exp(1 - r/c) if c > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate(candidate, reference, max_n=4, weights=None):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score\n",
    "        \n",
    "        Args:\n",
    "            candidate: Generated translation\n",
    "            reference: Gold standard translation\n",
    "            max_n: Maximum n-gram size (default: 4)\n",
    "            weights: Weights for each n-gram (default: uniform)\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0/max_n] * max_n\n",
    "        \n",
    "        # Calculate modified precision for each n-gram\n",
    "        precisions = []\n",
    "        for n in range(1, max_n+1):\n",
    "            p = BLEUScore.modified_precision(candidate, reference, n)\n",
    "            precisions.append(p)\n",
    "        \n",
    "        # Geometric mean of precisions\n",
    "        if 0 in precisions:\n",
    "            geo_mean = 0.0\n",
    "        else:\n",
    "            log_sum = sum(w * math.log(p) for w, p in zip(weights, precisions))\n",
    "            geo_mean = math.exp(log_sum)\n",
    "        \n",
    "        # Apply brevity penalty\n",
    "        bp = BLEUScore.brevity_penalty(candidate, reference)\n",
    "        \n",
    "        return bp * geo_mean\n",
    "\n",
    "# Example evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"BLEU EVALUATION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reference = \"the cat is on the mat\"\n",
    "candidates = [\n",
    "    \"the cat is on the mat\",  # Perfect match\n",
    "    \"the cat sits on the mat\",  # Close match\n",
    "    \"a cat is on a mat\",  # Different articles\n",
    "    \"cat mat on\",  # Poor match\n",
    "]\n",
    "\n",
    "for i, candidate in enumerate(candidates, 1):\n",
    "    bleu = BLEUScore.calculate(candidate, reference)\n",
    "    print(f\"\\nCandidate {i}: {candidate}\")\n",
    "    print(f\"  BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Perplexity (Language Modeling)\n",
    "\n",
    "### Cell Purpose: Calculate perplexity for language models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityEvaluator:\n",
    "    \"\"\"Perplexity: How well model predicts a sample\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate(model, text_tokens, device):\n",
    "        \"\"\"\n",
    "        Calculate perplexity\n",
    "        \n",
    "        Perplexity = exp(average negative log-likelihood)\n",
    "        Lower perplexity = better model\n",
    "        \n",
    "        Args:\n",
    "            model: Language model\n",
    "            text_tokens: Tokenized text\n",
    "            device: torch device\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process in chunks\n",
    "            for i in range(len(text_tokens) - 1):\n",
    "                input_ids = torch.tensor([text_tokens[i:i+1]]).to(device)\n",
    "                target_ids = torch.tensor([text_tokens[i+1]]).to(device)\n",
    "                \n",
    "                # Get model predictions\n",
    "                logits = model(input_ids)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    target_ids.view(-1),\n",
    "                    reduction='sum'\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_tokens += 1\n",
    "        \n",
    "        # Perplexity = exp(average loss)\n",
    "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_batch(model, dataloader, device):\n",
    "        \"\"\"Calculate perplexity over a dataset\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in dataloader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                logits = model(batch_x)\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    batch_y.view(-1),\n",
    "                    reduction='sum'\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_tokens += batch_y.numel()\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return perplexity, avg_loss\n",
    "\n",
    "# Example: Compare model perplexities\n",
    "print(\"=\"*60)\n",
    "print(\"PERPLEXITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate perplexity scores for different models\n",
    "models_ppl = {\n",
    "    \"Random Baseline\": 50257,  # Vocab size (uniform distribution)\n",
    "    \"Bigram Model\": 1200,\n",
    "    \"Small GPT\": 150,\n",
    "    \"Medium GPT\": 45,\n",
    "    \"Large GPT\": 20,\n",
    "    \"GPT-3\": 15\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(models_ppl.keys())\n",
    "ppls = list(models_ppl.values())\n",
    "\n",
    "plt.barh(names, ppls, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Perplexity (lower is better)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Perplexity Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"   - Lower perplexity = better predictions\")\n",
    "print(\"   - Perplexity ~= vocabulary size means random guessing\")\n",
    "print(\"   - Good models: perplexity 10-50\")\n",
    "print(\"   - Excellent models: perplexity <20\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Comprehensive Evaluation Dashboard\n",
    "\n",
    "### Cell Purpose: Create evaluation dashboard for model comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDashboard:\n",
    "    \"\"\"Comprehensive evaluation dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_result(self, model_name, task, metric_name, score):\n",
    "        \"\"\"Add evaluation result\"\"\"\n",
    "        if model_name not in self.results:\n",
    "            self.results[model_name] = {}\n",
    "        if task not in self.results[model_name]:\n",
    "            self.results[model_name][task] = {}\n",
    "        \n",
    "        self.results[model_name][task][metric_name] = score\n",
    "    \n",
    "    def compare_models(self, task):\n",
    "        \"\"\"Compare models on a specific task\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(f\"MODEL COMPARISON: {task.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        models = list(self.results.keys())\n",
    "        metrics = list(self.results[models[0]][task].keys())\n",
    "        \n",
    "        # Print table\n",
    "        header = f\"{'Model':<20}\"\n",
    "        for metric in metrics:\n",
    "            header += f\"{metric:<15}\"\n",
    "        print(header)\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for model in models:\n",
    "            row = f\"{model:<20}\"\n",
    "            for metric in metrics:\n",
    "                score = self.results[model][task][metric]\n",
    "                row += f\"{score:<15.4f}\"\n",
    "            print(row)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def visualize_all(self):\n",
    "        \"\"\"Create comprehensive visualization\"\"\"\n",
    "        models = list(self.results.keys())\n",
    "        tasks = list(self.results[models[0]].keys())\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(tasks), figsize=(5*len(tasks), 5))\n",
    "        if len(tasks) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, task in enumerate(tasks):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Get scores for this task\n",
    "            metrics = list(self.results[models[0]][task].keys())\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.8 / len(metrics)\n",
    "            \n",
    "            for i, metric in enumerate(metrics):\n",
    "                scores = [self.results[m][task][metric] for m in models]\n",
    "                offset = (i - len(metrics)/2) * width\n",
    "                ax.bar(x + offset, scores, width, label=metric, alpha=0.7)\n",
    "            \n",
    "            ax.set_xlabel('Model', fontweight='bold')\n",
    "            ax.set_ylabel('Score', fontweight='bold')\n",
    "            ax.set_title(f'{task.title()} Performance', fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demo dashboard\n",
    "dashboard = EvaluationDashboard()\n",
    "\n",
    "# Add sample results\n",
    "dashboard.add_result(\"Model A\", \"summarization\", \"ROUGE-1\", 0.45)\n",
    "dashboard.add_result(\"Model A\", \"summarization\", \"ROUGE-2\", 0.22)\n",
    "dashboard.add_result(\"Model A\", \"translation\", \"BLEU\", 0.38)\n",
    "\n",
    "dashboard.add_result(\"Model B\", \"summarization\", \"ROUGE-1\", 0.52)\n",
    "dashboard.add_result(\"Model B\", \"summarization\", \"ROUGE-2\", 0.28)\n",
    "dashboard.add_result(\"Model B\", \"translation\", \"BLEU\", 0.42)\n",
    "\n",
    "# Compare\n",
    "dashboard.compare_models(\"summarization\")\n",
    "dashboard.visualize_all()\n",
    "\n",
    "print(\"âœ… Evaluation dashboard ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Production Monitoring\n",
    "\n",
    "### Cell Purpose: AWS CloudWatch integration for continuous evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_guide = \"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PRODUCTION MONITORING & EVALUATION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## Real-Time Metrics\n",
    "\n",
    "### 1. Model Performance Metrics\n",
    "--------------------------------------------------------------\n",
    "- **Perplexity**: Track on holdout set daily\n",
    "- **ROUGE/BLEU**: Sample evaluation on recent predictions\n",
    "- **Latency**: P50, P95, P99 response times\n",
    "- **Throughput**: Requests per second\n",
    "\n",
    "### 2. Data Quality Metrics\n",
    "--------------------------------------------------------------\n",
    "- **Input Length Distribution**: Detect unusual patterns\n",
    "- **Output Length Distribution**: Check for degeneration\n",
    "- **Token Distribution**: Monitor vocabulary usage\n",
    "- **Error Rates**: Track generation failures\n",
    "\n",
    "## AWS CloudWatch Implementation\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelMonitor:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.cloudwatch = boto3.client('cloudwatch')\n",
    "    \n",
    "    def log_metric(self, metric_name, value, unit='None'):\n",
    "        self.cloudwatch.put_metric_data(\n",
    "            Namespace='LLM/Evaluation',\n",
    "            MetricData=[{\n",
    "                'MetricName': metric_name,\n",
    "                'Value': value,\n",
    "                'Unit': unit,\n",
    "                'Timestamp': datetime.utcnow(),\n",
    "                'Dimensions': [\n",
    "                    {'Name': 'ModelName', 'Value': self.model_name}\n",
    "                ]\n",
    "            }]\n",
    "        )\n",
    "    \n",
    "    def log_inference(self, latency_ms, tokens_generated, success=True):\n",
    "        self.log_metric('InferenceLatency', latency_ms, 'Milliseconds')\n",
    "        self.log_metric('TokensGenerated', tokens_generated, 'Count')\n",
    "        self.log_metric('InferenceSuccess', 1 if success else 0, 'Count')\n",
    "    \n",
    "    def log_quality(self, rouge1, rouge2, bleu):\n",
    "        self.log_metric('ROUGE-1', rouge1, 'None')\n",
    "        self.log_metric('ROUGE-2', rouge2, 'None')\n",
    "        self.log_metric('BLEU', bleu, 'None')\n",
    "```\n",
    "\n",
    "### 3. Create CloudWatch Dashboard\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "# dashboard_config.json\n",
    "{\n",
    "  \"widgets\": [\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          [\"LLM/Evaluation\", \"ROUGE-1\", {\"stat\": \"Average\"}],\n",
    "          [\".\", \"ROUGE-2\", {\"stat\": \"Average\"}],\n",
    "          [\".\", \"BLEU\", {\"stat\": \"Average\"}]\n",
    "        ],\n",
    "        \"period\": 300,\n",
    "        \"stat\": \"Average\",\n",
    "        \"region\": \"us-east-1\",\n",
    "        \"title\": \"Model Quality Metrics\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"metric\",\n",
    "      \"properties\": {\n",
    "        \"metrics\": [\n",
    "          [\"LLM/Evaluation\", \"InferenceLatency\", {\"stat\": \"Average\"}]\n",
    "        ],\n",
    "        \"period\": 60,\n",
    "        \"stat\": \"Average\",\n",
    "        \"region\": \"us-east-1\",\n",
    "        \"title\": \"Inference Latency (ms)\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Automated Quality Checks\n",
    "\n",
    "### Daily Evaluation Script\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "# daily_eval.py\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def daily_evaluation():\n",
    "    # Load test set\n",
    "    test_set = load_test_data()\n",
    "    \n",
    "    # Evaluate model\n",
    "    rouge_scores = evaluate_rouge(model, test_set)\n",
    "    bleu_scores = evaluate_bleu(model, test_set)\n",
    "    perplexity = calculate_perplexity(model, test_set)\n",
    "    \n",
    "    # Log to CloudWatch\n",
    "    monitor = ModelMonitor('production-model')\n",
    "    monitor.log_metric('DailyRouge1', rouge_scores['rouge1'])\n",
    "    monitor.log_metric('DailyBLEU', bleu_scores)\n",
    "    monitor.log_metric('DailyPerplexity', perplexity)\n",
    "    \n",
    "    # Alert if metrics degrade\n",
    "    if rouge_scores['rouge1'] < THRESHOLD:\n",
    "        send_alert(\"Model quality degraded!\")\n",
    "\n",
    "# Schedule daily\n",
    "schedule.every().day.at(\"02:00\").do(daily_evaluation)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(3600)\n",
    "```\n",
    "\n",
    "## Alerting Rules\n",
    "\n",
    "### CloudWatch Alarms\n",
    "--------------------------------------------------------------\n",
    "```python\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Alert if ROUGE-1 drops below threshold\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName='LLM-Quality-Drop',\n",
    "    MetricName='ROUGE-1',\n",
    "    Namespace='LLM/Evaluation',\n",
    "    Statistic='Average',\n",
    "    Period=3600,\n",
    "    EvaluationPeriods=2,\n",
    "    Threshold=0.40,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    AlarmActions=['arn:aws:sns:us-east-1:123456789:alerts']\n",
    ")\n",
    "\n",
    "# Alert if latency exceeds threshold\n",
    "cloudwatch.put_metric_alarm(\n",
    "    AlarmName='LLM-High-Latency',\n",
    "    MetricName='InferenceLatency',\n",
    "    Namespace='LLM/Evaluation',\n",
    "    Statistic='Average',\n",
    "    Period=300,\n",
    "    EvaluationPeriods=1,\n",
    "    Threshold=500,  # 500ms\n",
    "    ComparisonOperator='GreaterThanThreshold'\n",
    ")\n",
    "```\n",
    "\n",
    "## Cost for Monitoring\n",
    "--------------------------------------------------------------\n",
    "- CloudWatch metrics: $0.30 per metric/month\n",
    "- CloudWatch dashboards: $3.00/month\n",
    "- CloudWatch alarms: $0.10 per alarm/month\n",
    "- Total: ~$5-10/month for comprehensive monitoring\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(monitoring_guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Chapter Summary\n",
    "\n",
    "### What We Built:\n",
    "1. âœ… **ROUGE Score**: Summarization evaluation from scratch\n",
    "2. âœ… **BLEU Score**: Translation quality metric\n",
    "3. âœ… **Perplexity**: Language modeling capability\n",
    "4. âœ… **Evaluation Dashboard**: Compare multiple models\n",
    "5. âœ… **Production Monitoring**: CloudWatch integration\n",
    "6. âœ… **Automated Alerts**: Quality degradation detection\n",
    "\n",
    "### Metrics Overview:\n",
    "\n",
    "| Metric | Use Case | Range | Better Score |\n",
    "|--------|----------|-------|--------------|\n",
    "| ROUGE-1 | Summarization | 0-1 | Higher |\n",
    "| ROUGE-2 | Summarization | 0-1 | Higher |\n",
    "| ROUGE-L | Summarization | 0-1 | Higher |\n",
    "| BLEU | Translation | 0-1 | Higher |\n",
    "| Perplexity | Language Model | 1-âˆž | Lower |\n",
    "\n",
    "### When to Use Each Metric:\n",
    "\n",
    "**ROUGE (Recall-Oriented)**\n",
    "- Best for: Summarization\n",
    "- Measures: N-gram overlap with reference\n",
    "- Variants: ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE-L (LCS)\n",
    "\n",
    "**BLEU (Precision-Oriented)**\n",
    "- Best for: Translation\n",
    "- Measures: Modified n-gram precision\n",
    "- Note: Requires exact word matches\n",
    "\n",
    "**Perplexity**\n",
    "- Best for: Language modeling\n",
    "- Measures: How \"surprised\" model is by text\n",
    "- Lower = better predictions\n",
    "\n",
    "**Human Evaluation**\n",
    "- Gold standard for quality\n",
    "- Expensive and time-consuming\n",
    "- Use for final validation\n",
    "\n",
    "### Production Monitoring Strategy:\n",
    "\n",
    "1. **Continuous Metrics**:\n",
    "   - Latency (every request)\n",
    "   - Throughput (every minute)\n",
    "   - Error rates (every request)\n",
    "\n",
    "2. **Daily Batch Evaluation**:\n",
    "   - ROUGE/BLEU on sample (100-1000 examples)\n",
    "   - Perplexity on validation set\n",
    "   - Distribution checks\n",
    "\n",
    "3. **Weekly Deep Dive**:\n",
    "   - Manual inspection of outputs\n",
    "   - Edge case testing\n",
    "   - Bias/toxicity checks\n",
    "\n",
    "4. **Monthly Reviews**:\n",
    "   - Full test set evaluation\n",
    "   - Human evaluation sample\n",
    "   - Model comparison\n",
    "\n",
    "### Cost Summary:\n",
    "- **Metrics Computation**: Free (run locally or on training instances)\n",
    "- **CloudWatch Monitoring**: ~$5-10/month\n",
    "- **Automated Evaluation**: ~$2-5/month (spot instances)\n",
    "- **Total**: ~$7-15/month for comprehensive monitoring\n",
    "\n",
    "### Best Practices:\n",
    "1. **Multiple Metrics**: No single metric tells the full story\n",
    "2. **Task-Specific**: Choose metrics appropriate for your task\n",
    "3. **Baseline**: Always compare against baseline models\n",
    "4. **Trends**: Monitor changes over time, not absolute values\n",
    "5. **Human Eval**: Sample-based human evaluation for final validation\n",
    "\n",
    "### Next Steps:\n",
    "âž¡ï¸ **Advanced Evaluation**: BERTScore, METEOR, CIDEr  \n",
    "âž¡ï¸ **Bias Detection**: Fairness and toxicity metrics  \n",
    "âž¡ï¸ **User Feedback**: Incorporate real user ratings  \n",
    "âž¡ï¸ **Continuous Improvement**: Retrain based on metrics  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Resources\n",
    "\n",
    "**Papers:**\n",
    "- [ROUGE Paper](https://aclanthology.org/W04-1013/)\n",
    "- [BLEU Paper](https://aclanthology.org/P02-1040/)\n",
    "- [BERTScore](https://arxiv.org/abs/1904.09675)\n",
    "- [Beyond Accuracy](https://arxiv.org/abs/2202.00666) - Holistic evaluation\n",
    "\n",
    "**Tools:**\n",
    "- [rouge-score](https://github.com/google-research/google-research/tree/master/rouge) - Google's implementation\n",
    "- [sacrebleu](https://github.com/mjpost/sacrebleu) - Standard BLEU\n",
    "- [nlg-eval](https://github.com/Maluuba/nlg-eval) - Multiple metrics\n",
    "- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - Comprehensive suite\n",
    "\n",
    "**AWS Documentation:**\n",
    "- [CloudWatch Custom Metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html)\n",
    "- [CloudWatch Dashboards](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html)\n",
    "- [CloudWatch Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You've completed the complete LLM series from fundamentals to production! ðŸŽ‰**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
