{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Working with Text Data & Tokenization\n",
        "\n",
        "**Portfolio Project: Building LLMs from Scratch on AWS** üìù\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/02_Text_Data_Tokenization.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Chapter Overview\n",
        "\n",
        "Learn how to process text data for LLM training:\n",
        "- Text tokenization methods (BPE, WordPiece)\n",
        "- Creating and managing vocabularies\n",
        "- Token embeddings and positional encodings\n",
        "- Building efficient data loaders\n",
        "- AWS S3 integration for datasets\n",
        "\n",
        "**Learning Objectives:**\n",
        "‚úÖ Implement tokenization from scratch  \n",
        "‚úÖ Understand BPE algorithm  \n",
        "‚úÖ Create custom vocabularies  \n",
        "‚úÖ Build PyTorch data loaders  \n",
        "‚úÖ Store/retrieve datasets from S3  \n",
        "\n",
        "**AWS Services:** S3 (data storage)  \n",
        "**Estimated Cost:** $0.10 - $1.00\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Setup\n",
        "\n",
        "### Cell Purpose: Install packages and configure AWS SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "IN_SAGEMAKER = '/opt/ml' in sys.executable\n",
        "\n",
        "print(f\"Environment: {'Colab' if IN_COLAB else 'SageMaker' if IN_SAGEMAKER else 'Local'}\")\n",
        "\n",
        "if IN_COLAB or IN_SAGEMAKER:\n",
        "    !pip install -q torch tiktoken matplotlib numpy pandas boto3 s3fs\n",
        "    print(\"‚úÖ Packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Import libraries and verify installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "import re\n",
        "from collections import Counter\n",
        "from importlib.metadata import version\n",
        "import json\n",
        "\n",
        "# Try to import AWS SDK\n",
        "try:\n",
        "    import boto3\n",
        "    import s3fs\n",
        "    AWS_AVAILABLE = True\n",
        "    print(\"‚úÖ AWS SDK available\")\n",
        "except ImportError:\n",
        "    AWS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  AWS SDK not available (optional)\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LIBRARY VERSIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch: {version('torch')}\")\n",
        "print(f\"Tiktoken: {version('tiktoken')}\")\n",
        "print(f\"NumPy: {version('numpy')}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Understanding Tokenization\n",
        "\n",
        "Tokenization breaks text into smaller units (tokens) that can be processed by neural networks.\n",
        "\n",
        "### Why Tokenization?\n",
        "- Neural networks work with numbers, not text\n",
        "- Need to convert words/characters into numeric IDs\n",
        "- Balance between vocabulary size and granularity\n",
        "\n",
        "### Tokenization Methods:\n",
        "1. **Character-level**: ['H', 'e', 'l', 'l', 'o']\n",
        "   - ‚úÖ Small vocabulary\n",
        "   - ‚ùå Long sequences\n",
        "   \n",
        "2. **Word-level**: ['Hello', ',', 'world', '!']\n",
        "   - ‚úÖ Semantic meaning\n",
        "   - ‚ùå Large vocabulary, OOV issues\n",
        "   \n",
        "3. **Subword (BPE)**: ['Hello', ',', 'world', '!']\n",
        "   - ‚úÖ Balance of both\n",
        "   - ‚úÖ Handles rare words\n",
        "   - ‚úÖ Used by GPT, BERT\n",
        "\n",
        "### Byte Pair Encoding (BPE):\n",
        "- Iteratively merges most frequent character pairs\n",
        "- Creates a vocabulary of subword units\n",
        "- Used by GPT-2, GPT-3, GPT-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Demonstrate different tokenization approaches on sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different tokenization methods\n",
        "sample_text = \"Hello, world! This is a tokenization example.\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TOKENIZATION COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Character-level\n",
        "char_tokens = list(sample_text)\n",
        "print(f\"\\n1. Character-level ({len(char_tokens)} tokens):\")\n",
        "print(f\"   {char_tokens[:20]}...\")\n",
        "\n",
        "# Word-level (simple)\n",
        "word_tokens = sample_text.split()\n",
        "print(f\"\\n2. Word-level ({len(word_tokens)} tokens):\")\n",
        "print(f\"   {word_tokens}\")\n",
        "\n",
        "# Subword (using tiktoken - GPT-2 tokenizer)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "subword_tokens = tokenizer.encode(sample_text)\n",
        "decoded_tokens = [tokenizer.decode([t]) for t in subword_tokens]\n",
        "\n",
        "print(f\"\\n3. BPE Subword ({len(subword_tokens)} tokens):\")\n",
        "print(f\"   Token IDs: {subword_tokens}\")\n",
        "print(f\"   Decoded: {decoded_tokens}\")\n",
        "\n",
        "# Vocabulary sizes\n",
        "print(f\"\\nüìä Vocabulary Sizes:\")\n",
        "print(f\"   Character-level: ~100-256\")\n",
        "print(f\"   Word-level: 50,000-100,000+\")\n",
        "print(f\"   BPE (GPT-2): {tokenizer.n_vocab}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Byte Pair Encoding (BPE) Implementation\n",
        "\n",
        "Let's implement BPE from scratch!\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. Start with character-level vocabulary\n",
        "2. Count all adjacent token pairs\n",
        "3. Merge most frequent pair\n",
        "4. Repeat until desired vocabulary size\n",
        "5. Create encoding/decoding functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement BPE tokenizer from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple BPE Implementation from Scratch\n",
        "class SimpleBPETokenizer:\n",
        "    '''Simple Byte Pair Encoding tokenizer implementation'''\n",
        "    \n",
        "    def __init__(self, vocab_size=300):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.merges = {}\n",
        "        self.vocab = {}\n",
        "        \n",
        "    def train(self, text):\n",
        "        '''Train BPE on text corpus'''\n",
        "        # Start with character-level tokens\n",
        "        words = text.split()\n",
        "        vocab = {' '.join(word) + ' _': 1 for word in words}\n",
        "        \n",
        "        # Iteratively merge pairs\n",
        "        for i in range(self.vocab_size - 256):  # Reserve space for base chars\n",
        "            pairs = self._get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "                \n",
        "            best = max(pairs, key=pairs.get)\n",
        "            vocab = self._merge_vocab(best, vocab)\n",
        "            self.merges[best] = i\n",
        "            \n",
        "            if (i + 1) % 50 == 0:\n",
        "                print(f\"   Merge {i+1}: {best}\")\n",
        "        \n",
        "        # Create vocabulary\n",
        "        self.vocab = {char: i for i, char in enumerate(set(''.join(vocab.keys())))}\n",
        "        \n",
        "    def _get_stats(self, vocab):\n",
        "        '''Count all adjacent pairs'''\n",
        "        pairs = {}\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols)-1):\n",
        "                pair = (symbols[i], symbols[i+1])\n",
        "                pairs[pair] = pairs.get(pair, 0) + freq\n",
        "        return pairs\n",
        "    \n",
        "    def _merge_vocab(self, pair, vocab):\n",
        "        '''Merge most frequent pair in vocabulary'''\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        \n",
        "        for word in vocab:\n",
        "            new_word = word.replace(bigram, replacement)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "        return new_vocab\n",
        "    \n",
        "    def encode(self, text):\n",
        "        '''Encode text to token IDs (simplified)'''\n",
        "        return [self.vocab.get(c, 0) for c in text]\n",
        "    \n",
        "    def decode(self, tokens):\n",
        "        '''Decode token IDs to text (simplified)'''\n",
        "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "        return ''.join([inv_vocab.get(t, '?') for t in tokens])\n",
        "\n",
        "# Train on sample text\n",
        "sample_corpus = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. \n",
        "Natural language processing is fascinating!\n",
        "Machine learning models can understand text.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüîß Training BPE Tokenizer...\")\n",
        "print(\"=\"*60)\n",
        "bpe = SimpleBPETokenizer(vocab_size=300)\n",
        "bpe.train(sample_corpus)\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Trained! Vocabulary size: {len(bpe.vocab)}\")\n",
        "\n",
        "# Test encoding/decoding\n",
        "test_text = \"fox jumps\"\n",
        "encoded = bpe.encode(test_text)\n",
        "decoded = bpe.decode(encoded)\n",
        "print(f\"\\nüìù Test: '{test_text}'\")\n",
        "print(f\"   Encoded: {encoded[:20]}...\")\n",
        "print(f\"   Decoded: '{decoded[:20]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Using Tiktoken (Production Tokenizer)\n",
        "\n",
        "For production use, we'll use `tiktoken` - OpenAI's BPE tokenizer.\n",
        "\n",
        "### Benefits:\n",
        "- ‚úÖ Fast (Rust implementation)\n",
        "- ‚úÖ Compatible with GPT models\n",
        "- ‚úÖ Well-tested and maintained\n",
        "- ‚úÖ Easy to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Work with tiktoken tokenizer and analyze vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Tiktoken (GPT-2/GPT-3 tokenizer)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "Large Language Models are transformer-based neural networks \n",
        "trained on vast amounts of text data. They learn to predict \n",
        "the next token in a sequence, which enables them to generate \n",
        "coherent and contextually relevant text.\n",
        "\"\"\"\n",
        "\n",
        "# Encode\n",
        "tokens = tokenizer.encode(text)\n",
        "print(\"=\"*60)\n",
        "print(\"TIKTOKEN TOKENIZATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original text length: {len(text)} characters\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"Compression ratio: {len(text)/len(tokens):.2f} chars/token\")\n",
        "\n",
        "# Show some tokens\n",
        "print(f\"\\nüìù First 20 tokens:\")\n",
        "for i in range(min(20, len(tokens))):\n",
        "    token_str = tokenizer.decode([tokens[i]])\n",
        "    print(f\"   {tokens[i]:5d} -> '{token_str}'\")\n",
        "\n",
        "# Decode back\n",
        "decoded = tokenizer.decode(tokens)\n",
        "print(f\"\\n‚úÖ Decoding matches original: {decoded.strip() == text.strip()}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Vocabulary info\n",
        "print(f\"\\nüìä GPT-2 Tokenizer Vocabulary:\")\n",
        "print(f\"   Total tokens: {tokenizer.n_vocab:,}\")\n",
        "print(f\"   Special tokens: <|endoftext|> (ID: {tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize token frequency and length distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token statistics\n",
        "longer_text = \"\"\"\n",
        "Artificial intelligence and machine learning have revolutionized \n",
        "how we process and understand natural language. Deep learning models,\n",
        "particularly transformers, have achieved remarkable success in tasks\n",
        "like translation, summarization, and question answering.\n",
        "\"\"\" * 5  # Repeat for better statistics\n",
        "\n",
        "tokens = tokenizer.encode(longer_text)\n",
        "token_strings = [tokenizer.decode([t]) for t in tokens]\n",
        "\n",
        "# Token frequency\n",
        "token_freq = Counter(tokens)\n",
        "most_common = token_freq.most_common(15)\n",
        "\n",
        "# Token length distribution\n",
        "token_lengths = [len(s) for s in token_strings]\n",
        "\n",
        "# Create visualizations\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Most common tokens\n",
        "tokens_str = [tokenizer.decode([t]) for t, _ in most_common]\n",
        "counts = [c for _, c in most_common]\n",
        "ax1.barh(range(len(counts)), counts, color='#3498db', edgecolor='black')\n",
        "ax1.set_yticks(range(len(tokens_str)))\n",
        "ax1.set_yticklabels([f\"'{t}'\" for t in tokens_str])\n",
        "ax1.set_xlabel('Frequency', fontweight='bold')\n",
        "ax1.set_title('Most Common Tokens', fontsize=13, fontweight='bold')\n",
        "ax1.invert_yaxis()\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Token length distribution\n",
        "ax2.hist(token_lengths, bins=range(1, max(token_lengths)+2), \n",
        "         color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "ax2.set_xlabel('Token Length (characters)', fontweight='bold')\n",
        "ax2.set_ylabel('Frequency', fontweight='bold')\n",
        "ax2.set_title('Token Length Distribution', fontsize=13, fontweight='bold')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä Statistics:\")\n",
        "print(f\"   Unique tokens: {len(set(tokens))}\")\n",
        "print(f\"   Avg token length: {np.mean(token_lengths):.2f} chars\")\n",
        "print(f\"   Total tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Token Embeddings\n",
        "\n",
        "Convert token IDs to dense vectors that neural networks can process.\n",
        "\n",
        "### Embedding Layer:\n",
        "- Maps each token ID to a learned vector\n",
        "- Typical dimensions: 128-12288 (GPT-4)\n",
        "- Trained end-to-end with the model\n",
        "\n",
        "### Why Embeddings?\n",
        "- Capture semantic relationships\n",
        "- Reduce dimensionality (vocab_size ‚Üí d_model)\n",
        "- Learn representations during training\n",
        "\n",
        "### Example:\n",
        "```\n",
        "Token ID 1234 ‚Üí [0.2, -0.5, 0.8, ..., 0.1]  # 768-dim vector\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Create and demonstrate token embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token Embedding Implementation\n",
        "class TokenEmbedding(nn.Module):\n",
        "    '''Simple token embedding layer'''\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Scale embeddings by sqrt(d_model) (as in original Transformer)\n",
        "        return self.embedding(x) * np.sqrt(self.d_model)\n",
        "\n",
        "# Create embedding layer\n",
        "vocab_size = tokenizer.n_vocab  # 50,257 for GPT-2\n",
        "d_model = 768  # GPT-2 embedding dimension\n",
        "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
        "\n",
        "# Example: embed some tokens\n",
        "example_text = \"Hello, world!\"\n",
        "token_ids = tokenizer.encode(example_text)\n",
        "token_tensor = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(token_tensor)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TOKEN EMBEDDINGS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input text: '{example_text}'\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"Token tensor shape: {token_tensor.shape}\")\n",
        "print(f\"Embedding output shape: {embeddings.shape}\")\n",
        "print(f\"   (batch_size=1, seq_len={len(token_ids)}, d_model={d_model})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize first embedding vector\n",
        "first_embedding = embeddings[0, 0, :].numpy()\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(first_embedding[:100], color='#3498db', linewidth=1.5)\n",
        "plt.xlabel('Dimension', fontweight='bold')\n",
        "plt.ylabel('Value', fontweight='bold')\n",
        "plt.title(f\"First Token Embedding (first 100 dims)\", fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(first_embedding, bins=50, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Embedding Value', fontweight='bold')\n",
        "plt.ylabel('Frequency', fontweight='bold')\n",
        "plt.title('Embedding Value Distribution', fontsize=12, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Embedding Statistics:\")\n",
        "print(f\"   Mean: {first_embedding.mean():.4f}\")\n",
        "print(f\"   Std: {first_embedding.std():.4f}\")\n",
        "print(f\"   Min: {first_embedding.min():.4f}\")\n",
        "print(f\"   Max: {first_embedding.max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Positional Encoding\n",
        "\n",
        "Transformers need position information since they process all tokens in parallel.\n",
        "\n",
        "### Why Positional Encoding?\n",
        "- Self-attention is permutation-invariant\n",
        "- Need to inject position information\n",
        "- Enables model to understand word order\n",
        "\n",
        "### Methods:\n",
        "1. **Sinusoidal (original Transformer)**: Fixed mathematical patterns\n",
        "2. **Learned (GPT, BERT)**: Trainable position embeddings\n",
        "\n",
        "### GPT-Style Learned Positional Embeddings:\n",
        "```python\n",
        "pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "final_embedding = token_embedding + pos_embedding\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement and visualize positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Positional Encoding Implementation\n",
        "class PositionalEncoding(nn.Module):\n",
        "    '''Learned positional encoding (GPT-style)'''\n",
        "    \n",
        "    def __init__(self, max_seq_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        return self.pos_embedding(positions)\n",
        "\n",
        "# Combined Token + Position Embeddings\n",
        "class InputEmbedding(nn.Module):\n",
        "    '''Complete input embedding: token + position'''\n",
        "    \n",
        "    def __init__(self, vocab_size, max_seq_len, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        token_emb = self.token_embedding(x)\n",
        "        pos_emb = self.pos_encoding(x)\n",
        "        return self.dropout(token_emb + pos_emb)\n",
        "\n",
        "# Create input embedding layer\n",
        "max_seq_len = 1024  # Maximum sequence length\n",
        "input_embedding = InputEmbedding(vocab_size, max_seq_len, d_model)\n",
        "\n",
        "# Test\n",
        "test_text = \"The transformer architecture uses self-attention.\"\n",
        "token_ids = torch.tensor(tokenizer.encode(test_text)).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    final_embeddings = input_embedding(token_ids)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"INPUT EMBEDDINGS (Token + Position)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input: '{test_text}'\")\n",
        "print(f\"Tokens: {token_ids.shape}\")\n",
        "print(f\"Final embeddings: {final_embeddings.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize positional encodings\n",
        "with torch.no_grad():\n",
        "    # Get first 50 position embeddings\n",
        "    pos_emb = input_embedding.pos_encoding(torch.arange(50).unsqueeze(0))\n",
        "    pos_matrix = pos_emb[0].numpy()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Heatmap of positional encodings\n",
        "plt.subplot(1, 2, 1)\n",
        "im = plt.imshow(pos_matrix[:, :100].T, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
        "plt.xlabel('Position', fontweight='bold')\n",
        "plt.ylabel('Embedding Dimension', fontweight='bold')\n",
        "plt.title('Positional Encoding Patterns (first 100 dims)', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(im, label='Value')\n",
        "\n",
        "# Individual position embeddings\n",
        "plt.subplot(1, 2, 2)\n",
        "for pos in [0, 10, 20, 30, 40]:\n",
        "    plt.plot(pos_matrix[pos, :100], label=f'Position {pos}', alpha=0.7, linewidth=2)\n",
        "plt.xlabel('Embedding Dimension', fontweight='bold')\n",
        "plt.ylabel('Value', fontweight='bold')\n",
        "plt.title('Position Embeddings at Different Positions', fontsize=12, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Positional encodings provide unique position information for each token!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Data Loading for LLM Training\n",
        "\n",
        "Efficient data loading is crucial for training LLMs.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Batch Processing**: Process multiple sequences simultaneously\n",
        "2. **Sequence Length**: Fixed length (e.g., 1024 tokens)\n",
        "3. **Padding/Truncation**: Handle variable-length sequences\n",
        "4. **Data Loaders**: PyTorch DataLoader for efficient batching\n",
        "\n",
        "### Training Data Format:\n",
        "```python\n",
        "Input:  \"The cat sat on\"\n",
        "Target: \"cat sat on the\"  # Shifted by 1 token\n",
        "```\n",
        "\n",
        "### Causal Language Modeling:\n",
        "- Predict next token given previous tokens\n",
        "- Autoregressive generation\n",
        "- Self-supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Create custom dataset and dataloader for LLM training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset for LLM Training\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    '''Dataset for causal language modeling'''\n",
        "    \n",
        "    def __init__(self, text, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Tokenize entire text\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "        \n",
        "        # Calculate number of samples\n",
        "        self.num_samples = len(self.tokens) // max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Get chunk of tokens\n",
        "        start_idx = idx * self.max_length\n",
        "        end_idx = start_idx + self.max_length\n",
        "        \n",
        "        chunk = self.tokens[start_idx:end_idx]\n",
        "        \n",
        "        # Input and target (shifted by 1)\n",
        "        x = torch.tensor(chunk[:-1])\n",
        "        y = torch.tensor(chunk[1:])\n",
        "        \n",
        "        return x, y\n",
        "\n",
        "# Create sample dataset\n",
        "sample_text = \"\"\"\n",
        "The transformer architecture revolutionized natural language processing.\n",
        "It uses self-attention mechanisms to process input sequences in parallel.\n",
        "This enables efficient training on large datasets and captures long-range dependencies.\n",
        "Models like GPT and BERT are based on this architecture.\n",
        "\"\"\" * 10  # Repeat for more data\n",
        "\n",
        "dataset = TextDataset(sample_text, tokenizer, max_length=64)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADER\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Text length: {len(sample_text)} characters\")\n",
        "print(f\"Total tokens: {len(tokenizer.encode(sample_text))}\")\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "print(f\"Sequence length: 64 tokens\")\n",
        "print(f\"Batch size: 4\")\n",
        "print(f\"Batches per epoch: {len(dataloader)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(dataloader))\n",
        "inputs, targets = sample_batch\n",
        "\n",
        "print(f\"\\nüì¶ Sample Batch:\")\n",
        "print(f\"   Input shape: {inputs.shape}\")\n",
        "print(f\"   Target shape: {targets.shape}\")\n",
        "\n",
        "# Show first sequence\n",
        "print(f\"\\nüìù First Sequence in Batch:\")\n",
        "first_input = inputs[0, :20]\n",
        "first_target = targets[0, :20]\n",
        "print(f\"   Input IDs: {first_input.tolist()}\")\n",
        "print(f\"   Target IDs: {first_target.tolist()}\")\n",
        "print(f\"\\n   Input text: {tokenizer.decode(first_input.tolist())}\")\n",
        "print(f\"   Target text: {tokenizer.decode(first_target.tolist())}\")\n",
        "print(f\"\\n   ‚úÖ Target is input shifted by 1 token!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize batch structure and data flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize batch structure\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "\n",
        "# Get a batch\n",
        "inputs, targets = next(iter(dataloader))\n",
        "\n",
        "# Plot 1: Token ID heatmap for inputs\n",
        "ax = axes[0, 0]\n",
        "im = ax.imshow(inputs.numpy(), aspect='auto', cmap='viridis', interpolation='nearest')\n",
        "ax.set_xlabel('Sequence Position', fontweight='bold')\n",
        "ax.set_ylabel('Batch Sample', fontweight='bold')\n",
        "ax.set_title('Input Token IDs (Batch=4, Seq=63)', fontsize=11, fontweight='bold')\n",
        "plt.colorbar(im, ax=ax, label='Token ID')\n",
        "\n",
        "# Plot 2: Token ID heatmap for targets\n",
        "ax = axes[0, 1]\n",
        "im = ax.imshow(targets.numpy(), aspect='auto', cmap='plasma', interpolation='nearest')\n",
        "ax.set_xlabel('Sequence Position', fontweight='bold')\n",
        "ax.set_ylabel('Batch Sample', fontweight='bold')\n",
        "ax.set_title('Target Token IDs (Shifted by 1)', fontsize=11, fontweight='bold')\n",
        "plt.colorbar(im, ax=ax, label='Token ID')\n",
        "\n",
        "# Plot 3: First sequence comparison\n",
        "ax = axes[1, 0]\n",
        "positions = range(20)\n",
        "ax.plot(positions, inputs[0, :20].numpy(), 'o-', label='Input', linewidth=2, markersize=6)\n",
        "ax.plot(positions, targets[0, :20].numpy(), 's-', label='Target', linewidth=2, markersize=6)\n",
        "ax.set_xlabel('Position', fontweight='bold')\n",
        "ax.set_ylabel('Token ID', fontweight='bold')\n",
        "ax.set_title('Input vs Target (First Sequence)', fontsize=11, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Plot 4: Data flow diagram\n",
        "ax = axes[1, 1]\n",
        "ax.axis('off')\n",
        "ax.text(0.5, 0.9, 'Data Flow for Next-Token Prediction', \n",
        "       ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "boxes = [\n",
        "    ('Raw Text', 0.75, '#3498db'),\n",
        "    ('Tokenize', 0.60, '#9b59b6'),\n",
        "    ('Create Sequences', 0.45, '#e67e22'),\n",
        "    ('Input & Target\\n(shifted)', 0.25, '#2ecc71'),\n",
        "    ('Model Training', 0.10, '#e74c3c')\n",
        "]\n",
        "\n",
        "for label, y, color in boxes:\n",
        "    rect = plt.Rectangle((0.2, y-0.05), 0.6, 0.08, \n",
        "                         facecolor=color, edgecolor='black', linewidth=2, alpha=0.7)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(0.5, y-0.01, label, ha='center', va='center', \n",
        "           fontsize=10, fontweight='bold', color='white')\n",
        "    \n",
        "    if y > 0.15:\n",
        "        ax.arrow(0.5, y-0.06, 0, -0.08, head_width=0.05, head_length=0.02, \n",
        "                fc='black', ec='black', linewidth=2)\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä The model learns by predicting the next token at each position!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 AWS S3 Integration\n",
        "\n",
        "Store and retrieve large datasets from AWS S3 for cost-effective storage.\n",
        "\n",
        "### Benefits:\n",
        "- **Scalable**: Store petabytes of data\n",
        "- **Cost-effective**: $0.023/GB/month\n",
        "- **Durable**: 99.999999999% durability\n",
        "- **Accessible**: From anywhere (SageMaker, EC2, local)\n",
        "\n",
        "### Usage Pattern:\n",
        "1. Upload datasets to S3\n",
        "2. Train models using S3 data\n",
        "3. Store checkpoints back to S3\n",
        "4. Version control with S3 prefixes\n",
        "\n",
        "### Cost Optimization:\n",
        "- Use S3 Standard for active data\n",
        "- Move to S3 Glacier for archival\n",
        "- Set lifecycle policies for auto-archiving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Demonstrate S3 data upload/download (requires AWS credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS S3 Integration (Optional - requires AWS credentials)\n",
        "def save_to_s3(data, bucket_name, key, use_s3=False):\n",
        "    '''Save data to S3 or local file'''\n",
        "    if use_s3 and AWS_AVAILABLE:\n",
        "        try:\n",
        "            s3_client = boto3.client('s3')\n",
        "            if isinstance(data, str):\n",
        "                s3_client.put_object(Bucket=bucket_name, Key=key, Body=data.encode())\n",
        "            else:\n",
        "                s3_client.put_object(Bucket=bucket_name, Key=key, Body=data)\n",
        "            print(f\"‚úÖ Uploaded to s3://{bucket_name}/{key}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå S3 upload failed: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        # Save locally\n",
        "        with open(key, 'w' if isinstance(data, str) else 'wb') as f:\n",
        "            f.write(data)\n",
        "        print(f\"üíæ Saved locally: {key}\")\n",
        "        return True\n",
        "\n",
        "def load_from_s3(bucket_name, key, use_s3=False):\n",
        "    '''Load data from S3 or local file'''\n",
        "    if use_s3 and AWS_AVAILABLE:\n",
        "        try:\n",
        "            s3_client = boto3.client('s3')\n",
        "            response = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
        "            data = response['Body'].read()\n",
        "            print(f\"‚úÖ Downloaded from s3://{bucket_name}/{key}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå S3 download failed: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # Load locally\n",
        "        try:\n",
        "            with open(key, 'r') as f:\n",
        "                data = f.read()\n",
        "            print(f\"üíæ Loaded locally: {key}\")\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Local load failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Example usage (local mode by default)\n",
        "print(\"=\"*60)\n",
        "print(\"DATA STORAGE EXAMPLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create sample dataset metadata\n",
        "dataset_info = {\n",
        "    \"name\": \"llm_training_data\",\n",
        "    \"num_samples\": len(dataset),\n",
        "    \"seq_length\": 64,\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"tokenizer\": \"gpt2\"\n",
        "}\n",
        "\n",
        "# Save to local file (or S3 if configured)\n",
        "save_to_s3(\n",
        "    json.dumps(dataset_info, indent=2),\n",
        "    bucket_name=\"your-llm-bucket\",  # Replace with your bucket\n",
        "    key=\"dataset_info.json\",\n",
        "    use_s3=False  # Set to True if AWS is configured\n",
        ")\n",
        "\n",
        "print(\"\\nüí° To use S3:\")\n",
        "print(\"   1. Configure AWS credentials (aws configure)\")\n",
        "print(\"   2. Create S3 bucket: aws s3 mb s3://your-llm-bucket\")\n",
        "print(\"   3. Set use_s3=True in the functions above\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Chapter Summary\n",
        "\n",
        "### What We Built:\n",
        "1. ‚úÖ **Tokenization**: BPE implementation and tiktoken usage\n",
        "2. ‚úÖ **Embeddings**: Token + positional embeddings\n",
        "3. ‚úÖ **Data Loading**: Custom Dataset and DataLoader\n",
        "4. ‚úÖ **S3 Integration**: Cloud storage for datasets\n",
        "5. ‚úÖ **Complete Pipeline**: Text ‚Üí Tokens ‚Üí Embeddings ‚Üí Batches\n",
        "\n",
        "### Key Takeaways:\n",
        "- Tokenization converts text to numeric representations\n",
        "- BPE balances vocabulary size and sequence length\n",
        "- Embeddings transform discrete tokens to continuous vectors\n",
        "- Positional encoding adds sequence order information\n",
        "- Efficient data loading is crucial for training\n",
        "\n",
        "### Implementation Highlights:\n",
        "```python\n",
        "# Complete pipeline\n",
        "text ‚Üí tokenizer.encode() ‚Üí token_ids\n",
        "token_ids ‚Üí embedding_layer() ‚Üí embeddings\n",
        "embeddings ‚Üí model ‚Üí predictions\n",
        "```\n",
        "\n",
        "### AWS Services Used:\n",
        "- **S3**: Dataset storage (~$0.023/GB/month)\n",
        "- **Estimated Cost**: $0.10 - $1.00 for this chapter\n",
        "\n",
        "### Next Steps:\n",
        "‚û°Ô∏è **Chapter 3**: Implement attention mechanisms - the core of transformers!\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Resources\n",
        "\n",
        "**Papers:**\n",
        "- [Byte Pair Encoding](https://arxiv.org/abs/1508.07909)\n",
        "- [SentencePiece](https://arxiv.org/abs/1808.06226)\n",
        "- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "**Documentation:**\n",
        "- [Tiktoken GitHub](https://github.com/openai/tiktoken)\n",
        "- [PyTorch Data Loading](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
        "- [AWS S3 Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html)\n",
        "\n",
        "**Ready for Chapter 3? Let's implement attention! üß†**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
