{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Attention Mechanisms - The Engine of Transformers\n",
        "\n",
        "**Portfolio Project: Building LLMs from Scratch on AWS** üß†\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/03_Attention_Mechanisms.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Chapter Overview\n",
        "\n",
        "Implement the attention mechanism - the core innovation of transformers:\n",
        "- Self-attention fundamentals\n",
        "- Scaled dot-product attention\n",
        "- Multi-head attention\n",
        "- Causal masking for autoregressive models\n",
        "- Efficient attention implementations\n",
        "\n",
        "**Learning Objectives:**\n",
        "‚úÖ Understand self-attention conceptually  \n",
        "‚úÖ Implement attention from scratch  \n",
        "‚úÖ Build multi-head attention  \n",
        "‚úÖ Apply causal masking  \n",
        "‚úÖ Optimize for efficiency  \n",
        "\n",
        "**AWS Services:** SageMaker Notebook  \n",
        "**Estimated Cost:** $0.10 - $0.50\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Setup\n",
        "\n",
        "### Cell Purpose: Install packages and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and import required packages\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "IN_SAGEMAKER = '/opt/ml' in sys.executable\n",
        "\n",
        "if IN_COLAB or IN_SAGEMAKER:\n",
        "    !pip install -q torch matplotlib numpy\n",
        "    print(\"‚úÖ Packages installed!\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from importlib.metadata import version\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ENVIRONMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch: {version('torch')}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Self-Attention Fundamentals\n",
        "\n",
        "Self-attention allows each token to attend to all other tokens in the sequence.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Query (Q)**: What am I looking for?\n",
        "- **Key (K)**: What do I contain?\n",
        "- **Value (V)**: What information do I provide?\n",
        "\n",
        "### Intuition:\n",
        "```\n",
        "\"The cat sat on the mat\"\n",
        "\n",
        "For token \"cat\":\n",
        "- Query: \"I need context about this animal\"\n",
        "- Keys: All other tokens advertise their content\n",
        "- Values: Actual information from relevant tokens\n",
        "- Attention: \"The\" and \"sat\" are most relevant to \"cat\"\n",
        "```\n",
        "\n",
        "### Mathematics:\n",
        "```\n",
        "Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V\n",
        "```\n",
        "\n",
        "Where:\n",
        "- Q, K, V are linear projections of input\n",
        "- ‚àöd_k is scaling factor (square root of key dimension)\n",
        "- softmax normalizes attention weights to sum to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement simple self-attention from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Self-Attention (no trainable weights)\n",
        "# Example sentence: \"Your journey starts with one step\"\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your\n",
        "   [0.55, 0.87, 0.66], # journey\n",
        "   [0.57, 0.85, 0.64], # starts\n",
        "   [0.22, 0.58, 0.33], # with\n",
        "   [0.77, 0.25, 0.10], # one\n",
        "   [0.05, 0.80, 0.55]] # step\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SIMPLE SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input shape: {inputs.shape}\")\n",
        "print(f\"Input (first 3 tokens):\\n{inputs[:3]}\\n\")\n",
        "\n",
        "# Step 1: Compute attention scores (using token 2 as query)\n",
        "query = inputs[1]  # \"journey\"\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "print(f\"Query token: 'journey' (index 1)\")\n",
        "print(f\"Attention scores: {attn_scores_2}\\n\")\n",
        "\n",
        "# Step 2: Normalize with softmax\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(f\"Attention weights (normalized):\")\n",
        "print(f\"{attn_weights_2}\")\n",
        "print(f\"Sum: {attn_weights_2.sum():.4f}\\n\")\n",
        "\n",
        "# Step 3: Compute context vector\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2[i] * x_i\n",
        "\n",
        "print(f\"Context vector for 'journey':\")\n",
        "print(f\"{context_vec_2}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize attention patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute attention for all tokens\n",
        "def compute_attention_scores(inputs):\n",
        "    \"\"\"Compute attention scores for all token pairs\"\"\"\n",
        "    attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
        "    \n",
        "    for i, query in enumerate(inputs):\n",
        "        for j, key in enumerate(inputs):\n",
        "            attn_scores[i, j] = torch.dot(query, key)\n",
        "    \n",
        "    return attn_scores\n",
        "\n",
        "def compute_attention_weights(attn_scores):\n",
        "    \"\"\"Normalize scores to weights using softmax\"\"\"\n",
        "    return torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "# Compute full attention matrix\n",
        "attn_scores = compute_attention_scores(inputs)\n",
        "attn_weights = compute_attention_weights(attn_scores)\n",
        "\n",
        "print(f\"Attention weight matrix shape: {attn_weights.shape}\")\n",
        "print(f\"\\nAttention weights:\\n{attn_weights}\\n\")\n",
        "\n",
        "# Visualize\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "tokens = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
        "\n",
        "# Heatmap\n",
        "im = ax1.imshow(attn_weights.numpy(), cmap='Blues')\n",
        "ax1.set_xticks(range(len(tokens)))\n",
        "ax1.set_yticks(range(len(tokens)))\n",
        "ax1.set_xticklabels(tokens, rotation=45)\n",
        "ax1.set_yticklabels(tokens)\n",
        "ax1.set_xlabel('Keys', fontweight='bold')\n",
        "ax1.set_ylabel('Queries', fontweight='bold')\n",
        "ax1.set_title('Attention Weight Matrix', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Add values\n",
        "for i in range(len(tokens)):\n",
        "    for j in range(len(tokens)):\n",
        "        text = ax1.text(j, i, f'{attn_weights[i, j].item():.2f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
        "\n",
        "plt.colorbar(im, ax=ax1, label='Attention Weight')\n",
        "\n",
        "# Bar chart for \"journey\"\n",
        "ax2.barh(tokens, attn_weights[1].numpy(), color='#3498db', edgecolor='black')\n",
        "ax2.set_xlabel('Attention Weight', fontweight='bold')\n",
        "ax2.set_title('Attention for \"journey\"', fontsize=13, fontweight='bold')\n",
        "ax2.invert_yaxis()\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Attention allows each token to gather context from all others!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Scaled Dot-Product Attention\n",
        "\n",
        "Add trainable parameters and scaling for better performance.\n",
        "\n",
        "### Improvements:\n",
        "1. **Learnable Projections**: W_q, W_k, W_v matrices\n",
        "2. **Scaling**: Divide by ‚àöd_k to prevent large values\n",
        "3. **Batch Processing**: Handle multiple sequences\n",
        "\n",
        "### Formula:\n",
        "```python\n",
        "Q = X @ W_q  # Query projection\n",
        "K = X @ W_k  # Key projection\n",
        "V = X @ W_v  # Value projection\n",
        "\n",
        "scores = (Q @ K.T) / sqrt(d_k)\n",
        "weights = softmax(scores)\n",
        "output = weights @ V\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement scaled dot-product attention with trainable weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaled Dot-Product Attention\n",
        "class SelfAttention_v1(nn.Module):\n",
        "    \"\"\"Self-attention with trainable projections\"\"\"\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        \n",
        "        # Scaled dot-product\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_scores = attn_scores / keys.shape[-1]**0.5\n",
        "        \n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        \n",
        "        context_vectors = attn_weights @ values\n",
        "        return context_vectors\n",
        "\n",
        "# Test\n",
        "torch.manual_seed(123)\n",
        "d_in, d_out = 3, 2\n",
        "\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "\n",
        "# Example input\n",
        "x = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],\n",
        "     [0.55, 0.87, 0.66],\n",
        "     [0.57, 0.85, 0.64],\n",
        "     [0.22, 0.58, 0.33],\n",
        "     [0.77, 0.25, 0.10],\n",
        "     [0.05, 0.80, 0.55]]\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SCALED DOT-PRODUCT ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {sa_v1(x).shape}\")\n",
        "print(f\"\\nOutput:\\n{sa_v1(x)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize learned attention\n",
        "with torch.no_grad():\n",
        "    queries = sa_v1.W_query(x)\n",
        "    keys = sa_v1.W_key(x)\n",
        "    attn_scores = queries @ keys.T / keys.shape[-1]**0.5\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "im = plt.imshow(attn_weights.numpy(), cmap='viridis')\n",
        "plt.colorbar(im, label='Attention Weight')\n",
        "plt.xlabel('Key Position', fontweight='bold')\n",
        "plt.ylabel('Query Position', fontweight='bold')\n",
        "plt.title('Learned Attention Weights', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Causal Self-Attention\n",
        "\n",
        "For autoregressive models (like GPT), we need to prevent attending to future tokens.\n",
        "\n",
        "### Why Causal Masking?\n",
        "- During training: Model shouldn't see future tokens\n",
        "- Simulates generation: Only past tokens available\n",
        "- Implementation: Mask with -inf before softmax\n",
        "\n",
        "### Mask Structure:\n",
        "```\n",
        "[[1, 0, 0, 0],   After softmax:  [[1.0, 0.0, 0.0, 0.0],\n",
        " [1, 1, 0, 0],    ->              [0.5, 0.5, 0.0, 0.0],\n",
        " [1, 1, 1, 0],                    [0.33, 0.33, 0.33, 0.0],\n",
        " [1, 1, 1, 1]]                    [0.25, 0.25, 0.25, 0.25]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement causal self-attention with masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal Self-Attention\n",
        "class CausalAttention(nn.Module):\n",
        "    \"\"\"Causal self-attention for autoregressive models\"\"\"\n",
        "    def __init__(self, d_in, d_out, context_length, dropout=0.0, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Create causal mask\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        \n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        \n",
        "        # Scaled attention scores\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores = attn_scores / keys.shape[-1]**0.5\n",
        "        \n",
        "        # Apply causal mask\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "        )\n",
        "        \n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "# Test\n",
        "torch.manual_seed(123)\n",
        "context_length = 6\n",
        "ca = CausalAttention(d_in=3, d_out=2, context_length=context_length, dropout=0.1)\n",
        "\n",
        "# Batch of sequences\n",
        "batch = x.unsqueeze(0)  # Add batch dimension\n",
        "print(\"=\"*60)\n",
        "print(\"CAUSAL ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input shape: {batch.shape}\")\n",
        "print(f\"Output shape: {ca(batch).shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize causal mask\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Raw mask\n",
        "mask = torch.triu(torch.ones(6, 6), diagonal=1)\n",
        "ax1.imshow(mask.numpy(), cmap='Greys', vmin=0, vmax=1)\n",
        "ax1.set_title('Causal Mask\\n(1 = masked)', fontsize=11, fontweight='bold')\n",
        "ax1.set_xlabel('Key Position')\n",
        "ax1.set_ylabel('Query Position')\n",
        "\n",
        "# Attention scores before softmax\n",
        "with torch.no_grad():\n",
        "    queries = ca.W_query(batch)\n",
        "    keys = ca.W_key(batch)\n",
        "    scores = queries @ keys.transpose(1, 2) / keys.shape[-1]**0.5\n",
        "    scores_masked = scores.clone()\n",
        "    scores_masked.masked_fill_(mask.bool(), -torch.inf)\n",
        "\n",
        "im2 = ax2.imshow(scores[0].numpy(), cmap='RdBu_r', aspect='auto')\n",
        "ax2.set_title('Scores Before Mask', fontsize=11, fontweight='bold')\n",
        "ax2.set_xlabel('Key Position')\n",
        "plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "im3 = ax3.imshow(scores_masked[0].numpy(), cmap='RdBu_r', aspect='auto', vmin=-10, vmax=2)\n",
        "ax3.set_title('Scores After Mask\\n(-inf for future)', fontsize=11, fontweight='bold')\n",
        "ax3.set_xlabel('Key Position')\n",
        "plt.colorbar(im3, ax=ax3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîí Causal masking ensures we only attend to past tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Multi-Head Attention\n",
        "\n",
        "Instead of one attention mechanism, use multiple \"heads\" in parallel.\n",
        "\n",
        "### Benefits:\n",
        "- **Diverse Representations**: Each head learns different patterns\n",
        "- **Parallel Processing**: Computed simultaneously\n",
        "- **Better Performance**: Empirically superior\n",
        "\n",
        "### Architecture:\n",
        "```\n",
        "Input ‚Üí [Head 1, Head 2, ..., Head h] ‚Üí Concat ‚Üí Linear ‚Üí Output\n",
        "```\n",
        "\n",
        "### Example:\n",
        "- 12 heads with d_model=768\n",
        "- Each head: 768/12 = 64 dimensions\n",
        "- Concatenated: 12 √ó 64 = 768 dimensions\n",
        "- Final projection: 768 ‚Üí 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Implement complete multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#####################################\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        \n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        \n",
        "        # QKV projections\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        \n",
        "        # Split into heads\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose for attention\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        \n",
        "        # Apply causal mask\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "        \n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        # Combine heads\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        \n",
        "        return context_vec\n",
        "\n",
        "\n",
        "#####################################\n",
        "\n",
        "\n",
        "# Test Multi-Head Attention\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size = 2\n",
        "context_length = 6\n",
        "d_in = 3\n",
        "\n",
        "# Create sample batch\n",
        "batch = torch.randn(batch_size, context_length, d_in)\n",
        "\n",
        "# Multi-head attention parameters\n",
        "d_out = 8\n",
        "num_heads = 2\n",
        "\n",
        "mha = MultiHeadAttention(\n",
        "    d_in=d_in,\n",
        "    d_out=d_out,\n",
        "    context_length=context_length,\n",
        "    dropout=0.1,\n",
        "    num_heads=num_heads,\n",
        "    qkv_bias=False\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-HEAD ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input shape: {batch.shape}\")\n",
        "print(f\"Number of heads: {num_heads}\")\n",
        "print(f\"Head dimension: {d_out // num_heads}\")\n",
        "print(f\"Output shape: {mha(batch).shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize multi-head attention patterns\n",
        "with torch.no_grad():\n",
        "    # Get attention weights from each head\n",
        "    b, num_tokens, d = batch.shape\n",
        "    \n",
        "    queries = mha.W_query(batch)\n",
        "    keys = mha.W_key(batch)\n",
        "    \n",
        "    # Reshape for heads\n",
        "    queries = queries.view(b, num_tokens, num_heads, mha.head_dim).transpose(1, 2)\n",
        "    keys = keys.view(b, num_tokens, num_heads, mha.head_dim).transpose(1, 2)\n",
        "    \n",
        "    # Compute attention for each head\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    mask = mha.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "# Plot attention from each head\n",
        "fig, axes = plt.subplots(1, num_heads, figsize=(12, 5))\n",
        "if num_heads == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for head_idx in range(num_heads):\n",
        "    ax = axes[head_idx]\n",
        "    im = ax.imshow(attn_weights[0, head_idx].numpy(), cmap='viridis', aspect='auto')\n",
        "    ax.set_title(f'Head {head_idx+1}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Key Position')\n",
        "    ax.set_ylabel('Query Position')\n",
        "    plt.colorbar(im, ax=ax, label='Attention')\n",
        "\n",
        "plt.suptitle('Multi-Head Attention Patterns (First Sample)', \n",
        "             fontsize=14, fontweight='bold', y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Each head learns different attention patterns!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Chapter Summary\n",
        "\n",
        "### What We Implemented:\n",
        "1. ‚úÖ **Self-Attention**: Basic attention mechanism\n",
        "2. ‚úÖ **Scaled Attention**: With trainable projections\n",
        "3. ‚úÖ **Causal Masking**: For autoregressive models\n",
        "4. ‚úÖ **Multi-Head**: Parallel attention heads\n",
        "5. ‚úÖ **Complete Implementation**: Production-ready code\n",
        "\n",
        "### Key Insights:\n",
        "- Attention allows dynamic context aggregation\n",
        "- Scaling prevents numerical instability\n",
        "- Causal masking enables autoregressive training\n",
        "- Multi-head provides diverse representations\n",
        "\n",
        "### Mathematical Summary:\n",
        "```python\n",
        "# For each head h:\n",
        "Q_h = X @ W_q_h\n",
        "K_h = X @ W_k_h\n",
        "V_h = X @ W_v_h\n",
        "\n",
        "Attention_h = softmax(Q_h K_h^T / ‚àöd_k) V_h\n",
        "\n",
        "# Combine heads:\n",
        "MultiHead = Concat(Attention_1, ..., Attention_h) @ W_o\n",
        "```\n",
        "\n",
        "### Performance Characteristics:\n",
        "- Time Complexity: O(n¬≤d) where n=sequence length\n",
        "- Space Complexity: O(n¬≤) for attention matrix\n",
        "- Parallelizable: All heads computed simultaneously\n",
        "\n",
        "### Next Steps:\n",
        "‚û°Ô∏è **Chapter 4**: Combine attention with feed-forward networks to build the complete GPT model!\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Resources\n",
        "\n",
        "**Papers:**\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "**Ready for Chapter 4? Let's build GPT! ü§ñ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
