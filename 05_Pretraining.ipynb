{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5: Pretraining on Unlabeled Data\n",
        "\n",
        "**Portfolio Project: Building LLMs from Scratch on AWS** ðŸš€\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/05_Pretraining.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ Chapter Overview\n",
        "\n",
        "Train the GPT model from scratch:\n",
        "- Training loop implementation\n",
        "- Loss calculation\n",
        "- Optimization\n",
        "- Model evaluation\n",
        "- AWS SageMaker integration\n",
        "- Cost-effective training strategies\n",
        "\n",
        "**AWS Services:** SageMaker Training, S3, Spot Instances  \n",
        "**Estimated Cost:** $5-20\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Setup\n",
        "\n",
        "### Cell Purpose: Install dependencies and configure AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q torch tiktoken matplotlib tqdm boto3\n",
        "    \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "print(\"âœ… Environment ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Define GPT model architecture (from Chapter 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT Model Implementation (simplified from Chapter 4)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        \n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        \n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
        "        return self.out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
        "        \n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(\n",
        "                config[\"emb_dim\"],\n",
        "                config[\"n_heads\"],\n",
        "                config[\"emb_dim\"] * 4,\n",
        "                config[\"drop_rate\"]\n",
        "            )\n",
        "            for _ in range(config[\"n_layers\"])\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(config[\"emb_dim\"])\n",
        "        self.head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# GPT-124M Configuration\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1\n",
        "}\n",
        "\n",
        "print(\"âœ… GPT Model defined!\")\n",
        "print(f\"   Config: {GPT_CONFIG_124M['n_layers']} layers, {GPT_CONFIG_124M['emb_dim']} embedding dim\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Training Data Setup\n",
        "\n",
        "### Cell Purpose: Prepare training data and create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class for GPT training\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "        \n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Sample training data\n",
        "# For demonstration, using sample text. Replace with your own data file or load from S3\n",
        "text_data = \"\"\"\n",
        "Natural language processing has been revolutionized by large language models.\n",
        "These models are trained on massive amounts of text data using self-supervised learning.\n",
        "The transformer architecture, introduced in 2017, enabled unprecedented scaling of neural networks.\n",
        "Models like GPT, BERT, and their variants have achieved remarkable performance across many tasks.\n",
        "\n",
        "The key innovation is the self-attention mechanism, which allows the model to weigh the importance\n",
        "of different words in the input. This enables capturing long-range dependencies in text.\n",
        "Training these models requires substantial computational resources, often using GPUs or TPUs.\n",
        "\n",
        "Modern LLMs can perform various tasks including text generation, translation, summarization,\n",
        "question answering, and even code generation. They have become foundational tools in AI applications.\n",
        "Fine-tuning pre-trained models on specific tasks has become a standard approach in NLP.\n",
        "\n",
        "The scaling hypothesis suggests that model performance continues to improve with more parameters,\n",
        "more data, and more compute. This has driven the development of increasingly large models.\n",
        "However, efficiency and responsible AI practices are also important considerations.\n",
        "\"\"\" * 20  # Repeat to have enough data for training\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING DATA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Text length: {len(text_data):,} characters\")\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"Total tokens: {total_tokens:,}\")\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = GPTDataset(text_data, tokenizer, max_length=256, stride=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n",
        "\n",
        "print(f\"Dataset size: {len(train_dataset)} samples\")\n",
        "print(f\"Batches per epoch: {len(train_loader)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Training Functions\n",
        "\n",
        "### Cell Purpose: Define training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    \"\"\"Calculate loss for a single batch\"\"\"\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    \"\"\"Calculate average loss over multiple batches\"\"\"\n",
        "    total_loss = 0\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    \n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model(model, train_loader, optimizer, device, num_epochs, \n",
        "                eval_freq=10, eval_steps=5):\n",
        "    \"\"\"Train the model and track losses\"\"\"\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    step = 0\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for input_batch, target_batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            step += 1\n",
        "            \n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'step': step\n",
        "            })\n",
        "            \n",
        "            # Periodic evaluation\n",
        "            if step % eval_freq == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_loss = calc_loss_loader(train_loader, model, device, eval_steps)\n",
        "                train_losses.append((step, val_loss))\n",
        "                model.train()\n",
        "        \n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"âœ… Training complete!\")\n",
        "    return train_losses\n",
        "\n",
        "print(\"âœ… Training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Initialize model and run training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nðŸ“Š Model Info:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: ~{total_params * 4 / 1024**2:.1f} MB (float32)\")\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), \n",
        "    lr=5e-4,\n",
        "    weight_decay=0.1,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Starting training...\")\n",
        "train_losses = train_model(\n",
        "    model, \n",
        "    train_loader, \n",
        "    optimizer, \n",
        "    device, \n",
        "    num_epochs=10,\n",
        "    eval_freq=10,\n",
        "    eval_steps=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize training progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "if train_losses:\n",
        "    steps, losses = zip(*train_losses)\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Loss curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, losses, linewidth=2, color='#3498db', marker='o', markersize=4)\n",
        "    plt.xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "    plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    # Loss improvement\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if len(losses) > 1:\n",
        "        improvements = [losses[0] - l for l in losses]\n",
        "        plt.plot(steps, improvements, linewidth=2, color='#2ecc71', marker='s', markersize=4)\n",
        "        plt.xlabel('Training Steps', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Loss Improvement', fontsize=12, fontweight='bold')\n",
        "        plt.title('Loss Improvement from Start', fontsize=14, fontweight='bold')\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"ðŸ“Š Training Summary:\")\n",
        "    print(f\"   Initial loss: {losses[0]:.4f}\")\n",
        "    print(f\"   Final loss: {losses[-1]:.4f}\")\n",
        "    print(f\"   Improvement: {losses[0] - losses[-1]:.4f}\")\n",
        "    print(f\"   Reduction: {(1 - losses[-1]/losses[0]) * 100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Text Generation\n",
        "\n",
        "### Cell Purpose: Test the trained model with text generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if needed\n",
        "            idx_cond = idx if idx.size(1) <= context_size else idx[:, -context_size:]\n",
        "            \n",
        "            # Get predictions\n",
        "            logits = model(idx_cond)\n",
        "            logits = logits[:, -1, :]  # Get last time step\n",
        "            \n",
        "            # Apply temperature\n",
        "            logits = logits / temperature\n",
        "            \n",
        "            # Top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            \n",
        "            # Sample from distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            \n",
        "            # Append to sequence\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "    \n",
        "    return idx\n",
        "\n",
        "# Test text generation\n",
        "print(\"=\"*60)\n",
        "print(\"TEXT GENERATION TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_prompts = [\n",
        "    \"Natural language processing\",\n",
        "    \"The transformer architecture\",\n",
        "    \"Machine learning models\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nðŸ“ Prompt: '{prompt}'\")\n",
        "    \n",
        "    # Encode prompt\n",
        "    encoded = tokenizer.encode(prompt)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Generate\n",
        "    generated = generate_text(\n",
        "        model,\n",
        "        encoded_tensor,\n",
        "        max_new_tokens=50,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "        temperature=0.8,\n",
        "        top_k=40\n",
        "    )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(generated[0].tolist())\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "print(\"\\nâœ… Generation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 AWS SageMaker Training\n",
        "\n",
        "### Cell Purpose: Configure and launch SageMaker training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS SageMaker Training Configuration (Optional - requires AWS setup)\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"AWS SAGEMAKER TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# SageMaker configuration\n",
        "sagemaker_config = {\n",
        "    \"instance_type\": \"ml.g4dn.xlarge\",  # 1 GPU, 4 vCPUs, 16 GB RAM\n",
        "    \"instance_count\": 1,\n",
        "    \"use_spot_instances\": True,\n",
        "    \"max_run\": 7200,  # 2 hours max training\n",
        "    \"max_wait\": 14400,  # 4 hours max wait for spot\n",
        "    \"volume_size\": 30,  # GB\n",
        "    \"hyperparameters\": {\n",
        "        \"epochs\": 20,\n",
        "        \"batch-size\": 8,\n",
        "        \"learning-rate\": 5e-4,\n",
        "        \"max-seq-length\": 256,\n",
        "        \"model-size\": \"124M\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ“‹ SageMaker Configuration:\")\n",
        "print(json.dumps(sagemaker_config, indent=2))\n",
        "\n",
        "print(\"\\nðŸ’¡ Steps to Train on AWS SageMaker:\")\n",
        "print(\"   1. Prepare your training script (train.py)\")\n",
        "print(\"   2. Upload data to S3:\")\n",
        "print(\"      aws s3 cp your_data.txt s3://your-bucket/training-data/\")\n",
        "print(\"   3. Create PyTorch Estimator:\")\n",
        "print(\"      from sagemaker.pytorch import PyTorch\")\n",
        "print(\"      estimator = PyTorch(\")\n",
        "print(\"          entry_point='train.py',\")\n",
        "print(\"          instance_type='ml.g4dn.xlarge',\")\n",
        "print(\"          use_spot_instances=True,\")\n",
        "print(\"          ...)\")\n",
        "print(\"   4. Launch training:\")\n",
        "print(\"      estimator.fit({'training': 's3://your-bucket/training-data/'})\")\n",
        "print(\"   5. Monitor: Check AWS Console or use estimator.logs()\")\n",
        "\n",
        "# Cost estimation\n",
        "print(\"\\nðŸ’° Cost Estimation:\")\n",
        "on_demand_rate = 0.736  # g4dn.xlarge on-demand per hour\n",
        "spot_discount = 0.70  # Typical spot discount\n",
        "spot_rate = on_demand_rate * (1 - spot_discount)\n",
        "training_hours = 2\n",
        "\n",
        "print(f\"   Instance: ml.g4dn.xlarge (1x NVIDIA T4 GPU)\")\n",
        "print(f\"   On-Demand Rate: ${on_demand_rate:.3f}/hour\")\n",
        "print(f\"   Spot Rate (~70% off): ${spot_rate:.3f}/hour\")\n",
        "print(f\"\\n   Estimated Training Time: {training_hours} hours\")\n",
        "print(f\"   On-Demand Cost: ${on_demand_rate * training_hours:.2f}\")\n",
        "print(f\"   Spot Instance Cost: ${spot_rate * training_hours:.2f} âœ… Recommended\")\n",
        "print(f\"   Savings: ${(on_demand_rate - spot_rate) * training_hours:.2f}\")\n",
        "print(f\"\\n   Storage (30 GB): ~$0.14/hour\")\n",
        "print(f\"   Total Estimated Cost: ${spot_rate * training_hours + 0.14 * training_hours:.2f}\")\n",
        "\n",
        "print(\"\\nðŸ”‘ Tips for Cost Optimization:\")\n",
        "print(\"   â€¢ Use Spot Instances (save up to 70%)\")\n",
        "print(\"   â€¢ Start with smaller models for testing\")\n",
        "print(\"   â€¢ Monitor training closely to avoid overruns\")\n",
        "print(\"   â€¢ Use checkpointing to resume interrupted jobs\")\n",
        "print(\"   â€¢ Clean up resources after training\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Chapter Summary\n",
        "\n",
        "### What We Built:\n",
        "1. âœ… **GPT Model**: Full transformer implementation (124M parameters)\n",
        "2. âœ… **Training Dataset**: Custom dataset with sliding window approach\n",
        "3. âœ… **Training Loop**: Complete training with gradient clipping and evaluation\n",
        "4. âœ… **Text Generation**: Temperature and top-k sampling\n",
        "5. âœ… **AWS SageMaker**: Configuration for scalable cloud training\n",
        "6. âœ… **Cost Optimization**: Spot instances and best practices\n",
        "\n",
        "### Key Concepts:\n",
        "- **Causal Language Modeling**: Predict next token given previous context\n",
        "- **Cross-Entropy Loss**: Measures prediction accuracy\n",
        "- **Gradient Clipping**: Prevents exploding gradients\n",
        "- **Temperature Sampling**: Controls randomness in generation\n",
        "- **Top-k Sampling**: Filters unlikely tokens\n",
        "\n",
        "### Training Results:\n",
        "The model learns to:\n",
        "- Generate grammatically correct text\n",
        "- Follow semantic patterns from training data\n",
        "- Complete prompts coherently\n",
        "- Improve loss over training epochs\n",
        "\n",
        "### Implementation Highlights:\n",
        "```python\n",
        "# Training pipeline\n",
        "model = GPTModel(config)\n",
        "optimizer = AdamW(model.parameters())\n",
        "train_model(model, train_loader, optimizer, device, num_epochs)\n",
        "\n",
        "# Generation\n",
        "generated = generate_text(model, prompt, max_tokens=50, temperature=0.8)\n",
        "```\n",
        "\n",
        "### AWS Cost Breakdown:\n",
        "- **Local Training**: Free (CPU) or use personal GPU\n",
        "- **SageMaker Spot**: ~$0.50-2.00 for 2 hours training\n",
        "- **SageMaker On-Demand**: ~$1.50-3.00 for 2 hours\n",
        "- **S3 Storage**: ~$0.01/GB/month\n",
        "\n",
        "### Optimization Tips:\n",
        "1. **Start Small**: Test with fewer layers/parameters first\n",
        "2. **Use Spot Instances**: Save 60-70% on compute costs\n",
        "3. **Monitor Training**: Stop early if loss plateaus\n",
        "4. **Batch Size**: Larger batches = faster training (if memory allows)\n",
        "5. **Learning Rate**: 5e-4 works well for GPT-2 style models\n",
        "\n",
        "### Next Steps:\n",
        "âž¡ï¸ **Chapter 6**: Fine-tuning for specific tasks  \n",
        "âž¡ï¸ **Chapter 7**: Instruction tuning and alignment  \n",
        "âž¡ï¸ **Chapter 8**: Deployment and serving  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— Resources\n",
        "\n",
        "**Papers:**\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer\n",
        "- [GPT-2 Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/)\n",
        "- [Scaling Laws for Neural LMs](https://arxiv.org/abs/2001.08361)\n",
        "\n",
        "**AWS Documentation:**\n",
        "- [SageMaker Training](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html)\n",
        "- [Spot Instance Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)\n",
        "- [PyTorch on SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/)\n",
        "\n",
        "**Cost Optimization:**\n",
        "- [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
        "- [EC2 Spot Pricing](https://aws.amazon.com/ec2/spot/pricing/)\n",
        "- [S3 Pricing](https://aws.amazon.com/s3/pricing/)\n",
        "\n",
        "**Ready for fine-tuning? ðŸŽ¯**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
