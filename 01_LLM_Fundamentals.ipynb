{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 1: Understanding Large Language Models\n",
        "\n",
        "**Portfolio Project: Building LLMs from Scratch on AWS** üöÄ\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/llm-from-scratch-aws/blob/main/01_LLM_Fundamentals.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Chapter Overview\n",
        "\n",
        "This notebook covers foundational concepts of Large Language Models:\n",
        "- What are LLMs and how do they work?\n",
        "- Transformer architecture overview\n",
        "- LLM training stages: pretraining, finetuning, alignment\n",
        "- Model scale and computational requirements\n",
        "- AWS cost optimization strategies\n",
        "\n",
        "**Learning Objectives:**\n",
        "‚úÖ Understand core LLM concepts  \n",
        "‚úÖ Learn the LLM development lifecycle  \n",
        "‚úÖ Prepare for hands-on implementation  \n",
        "\n",
        "**AWS Services:** None (conceptual)  \n",
        "**Estimated Cost:** $0.00\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Setup\n",
        "        \n",
        "### Cell Purpose: Install and configure packages for AWS SageMaker and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for cloud environments\n",
        "import sys\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "IN_SAGEMAKER = '/opt/ml' in sys.executable or 'sagemaker' in sys.executable.lower()\n",
        "\n",
        "print(f\"Environment: {'Google Colab' if IN_COLAB else 'AWS SageMaker' if IN_SAGEMAKER else 'Local/Other'}\")\n",
        "\n",
        "# Install packages if needed\n",
        "if IN_COLAB or IN_SAGEMAKER:\n",
        "    !pip install -q torch matplotlib numpy pandas\n",
        "    print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Verify installations and check available compute resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries and verify installation\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from importlib.metadata import version\n",
        "import platform\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"PyTorch: {version('torch')}\")\n",
        "print(f\"NumPy: {version('numpy')}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 What is a Large Language Model?\n",
        "\n",
        "Large Language Models (LLMs) are deep learning models trained on massive text data to understand and generate human-like text.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Scale**: Billions of parameters (GPT-3: 175B, GPT-4: ~1.76T)\n",
        "- **Architecture**: Transformer-based (self-attention mechanisms)\n",
        "- **Training**: Unsupervised pretraining + supervised finetuning\n",
        "- **Capabilities**: Text generation, translation, summarization, Q&A, coding\n",
        "\n",
        "### LLM Development Stages:\n",
        "1. **Pretraining**: Learning language patterns from vast unlabeled text\n",
        "2. **Finetuning**: Adapting to specific tasks with labeled data\n",
        "3. **Alignment**: RLHF to align with human preferences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize and compare different LLM model sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize LLM scale comparison\n",
        "models = ['GPT-2\\nSmall', 'GPT-2\\nMedium', 'GPT-2\\nLarge', 'GPT-2\\nXL', 'GPT-3', 'GPT-4\\n(est)']\n",
        "parameters_billions = [0.124, 0.355, 0.774, 1.5, 175, 1760]\n",
        "colors = ['#3498db', '#5dade2', '#85c1e9', '#aed6f1', '#e74c3c', '#c0392b']\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "bars = plt.barh(models, parameters_billions, color=colors, edgecolor='black', linewidth=1.5)\n",
        "plt.xlabel('Parameters (Billions)', fontsize=12, fontweight='bold')\n",
        "plt.title('LLM Model Scale Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xscale('log')\n",
        "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "for bar, value in zip(bars, parameters_billions):\n",
        "    plt.text(value*1.3, bar.get_y() + bar.get_height()/2, \n",
        "             f'{value}B', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Training smaller models from scratch is practical and cost-effective!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Transformer Architecture\n",
        "\n",
        "The transformer is the foundation of modern LLMs.\n",
        "\n",
        "### Core Components:\n",
        "1. **Token Embeddings**: Convert text into dense vectors\n",
        "2. **Positional Encodings**: Add position information\n",
        "3. **Self-Attention**: Allow tokens to attend to each other\n",
        "4. **Multi-Head Attention**: Multiple attention in parallel\n",
        "5. **Feed-Forward Networks**: Process attended information\n",
        "6. **Layer Normalization**: Stabilize training\n",
        "7. **Residual Connections**: Enable deep networks\n",
        "\n",
        "### Architecture Types:\n",
        "- **GPT (Decoder-only)**: Autoregressive, left-to-right generation\n",
        "- **BERT (Encoder-only)**: Bidirectional, masked language modeling\n",
        "- **T5 (Encoder-Decoder)**: Full transformer, sequence-to-sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize the transformer architecture flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize transformer architecture\n",
        "fig, ax = plt.subplots(figsize=(10, 12))\n",
        "ax.axis('off')\n",
        "\n",
        "layers = [\n",
        "    ('Output Probabilities', 0.92, '#e74c3c'),\n",
        "    ('Linear + Softmax', 0.84, '#e67e22'),\n",
        "    ('Decoder Block N', 0.72, '#3498db'),\n",
        "    ('...', 0.64, '#95a5a6'),\n",
        "    ('Decoder Block 2', 0.56, '#3498db'),\n",
        "    ('Decoder Block 1', 0.48, '#3498db'),\n",
        "    ('Add Positional Encoding', 0.36, '#9b59b6'),\n",
        "    ('Token Embeddings', 0.28, '#1abc9c'),\n",
        "    ('Input Tokens', 0.16, '#2ecc71')\n",
        "]\n",
        "\n",
        "for label, y_pos, color in layers:\n",
        "    rect = plt.Rectangle((0.2, y_pos-0.04), 0.6, 0.06, \n",
        "                         facecolor=color, edgecolor='black', linewidth=2, alpha=0.7)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(0.5, y_pos-0.01, label, ha='center', va='center', \n",
        "           fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "for i in range(len(layers)-1):\n",
        "    y_start = layers[i][1] - 0.04\n",
        "    y_end = layers[i+1][1] + 0.02\n",
        "    ax.arrow(0.5, y_start, 0, y_end-y_start-0.01, \n",
        "            head_width=0.03, head_length=0.01, fc='black', ec='black', linewidth=2)\n",
        "\n",
        "ax.text(0.5, 0.98, 'GPT-Style Transformer (Decoder-Only)', \n",
        "       ha='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.87, 0.72, 'Self-Attention\\n+ FFN + Norm', \n",
        "       fontsize=8, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üèóÔ∏è We'll implement this architecture step-by-step in Chapters 2-4!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 LLM Training Pipeline\n",
        "\n",
        "### Stage 1: Pretraining (Chapter 5)\n",
        "- **Objective**: Learn general language understanding\n",
        "- **Data**: Large unlabeled text corpus\n",
        "- **Task**: Next-token prediction\n",
        "- **Duration**: Weeks to months\n",
        "- **Cost**: $100K - $10M+ for large models\n",
        "\n",
        "### Stage 2: Supervised Finetuning (Chapters 6-7)\n",
        "- **Objective**: Adapt to specific tasks\n",
        "- **Data**: Curated labeled datasets\n",
        "- **Task**: Task-specific objectives\n",
        "- **Duration**: Hours to days\n",
        "- **Cost**: $10 - $1,000\n",
        "\n",
        "### Stage 3: Alignment (Optional)\n",
        "- **Objective**: Align with human preferences\n",
        "- **Methods**: RLHF, DPO\n",
        "- **Duration**: Days to weeks\n",
        "- **Cost**: $1,000 - $100K+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize the three-stage training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training pipeline stages\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "stages = [\n",
        "    {'title': 'Stage 1:\\nPretraining', 'data': 'Unlabeled Text', \n",
        "     'task': 'Next Token\\nPrediction', 'output': 'Base Model', 'color': '#3498db'},\n",
        "    {'title': 'Stage 2:\\nFinetuning', 'data': 'Labeled Data', \n",
        "     'task': 'Task-Specific', 'output': 'Finetuned Model', 'color': '#e67e22'},\n",
        "    {'title': 'Stage 3:\\nAlignment', 'data': 'Human Feedback', \n",
        "     'task': 'RLHF / DPO', 'output': 'Aligned Model', 'color': '#2ecc71'}\n",
        "]\n",
        "\n",
        "for ax, stage in zip(axes, stages):\n",
        "    ax.axis('off')\n",
        "    ax.text(0.5, 0.95, stage['title'], ha='center', fontsize=13, \n",
        "           fontweight='bold', color=stage['color'])\n",
        "    \n",
        "    # Boxes and arrows\n",
        "    for y, text in [(0.75, stage['data']), (0.40, stage['task'])]:\n",
        "        rect = plt.Rectangle((0.1, y-0.1), 0.8, 0.15, \n",
        "                            facecolor=stage['color'], alpha=0.3, \n",
        "                            edgecolor=stage['color'], linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(0.5, y-0.025, text, ha='center', va='center', fontsize=9)\n",
        "        ax.arrow(0.5, y-0.12, 0, -0.13, head_width=0.08, head_length=0.05, \n",
        "                fc=stage['color'], ec=stage['color'], linewidth=2)\n",
        "    \n",
        "    rect = plt.Rectangle((0.1, 0.05), 0.8, 0.12, \n",
        "                        facecolor=stage['color'], alpha=0.7, \n",
        "                        edgecolor='black', linewidth=2)\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(0.5, 0.11, stage['output'], ha='center', va='center', \n",
        "           fontsize=10, fontweight='bold', color='white')\n",
        "    \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "plt.suptitle('LLM Training Pipeline', fontsize=15, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ We'll implement all three stages using cost-effective methods!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 AWS Cost Optimization\n",
        "\n",
        "### üí∞ Budget-Friendly LLM Development\n",
        "\n",
        "**Instance Selection:**\n",
        "- **Notebooks**: ml.t3.medium (CPU, $0.05/hr)\n",
        "- **Training**: ml.g4dn.xlarge (1 GPU, $0.526/hr)\n",
        "- **Spot Instances**: Save up to 70%!\n",
        "\n",
        "**Data Management:**\n",
        "- **S3**: $0.023/GB/month for datasets\n",
        "- **Lifecycle Policies**: Auto-delete old checkpoints\n",
        "\n",
        "**Training Optimization:**\n",
        "- Train GPT-2 scale models (124M-355M params)\n",
        "- Use FP16 mixed precision\n",
        "- Gradient accumulation for larger batches\n",
        "- Early stopping\n",
        "\n",
        "**Alternatives:**\n",
        "- **Google Colab**: Free GPU (T4, 15GB)\n",
        "- **Colab Pro**: $10/month (V100/A100)\n",
        "- **Local**: CPU-only for experiments\n",
        "\n",
        "### Estimated Costs:\n",
        "- **Development**: $5-20\n",
        "- **Full Training**: $20-50\n",
        "- **Complete Project**: **< $100 total**\n",
        "\n",
        "### Free Tier:\n",
        "- **AWS**: 250 hours SageMaker notebooks\n",
        "- **Google Colab**: Free GPU with limits\n",
        "- **AWS Educate**: $100-200 credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Calculate estimated AWS training costs for different scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AWS Cost Calculator for LLM Training\n",
        "def estimate_aws_cost(model_params_millions, epochs, hours_per_epoch, \n",
        "                      instance_type='g4dn.xlarge', use_spot=True):\n",
        "    '''Estimate AWS SageMaker training costs'''\n",
        "    \n",
        "    instance_prices = {\n",
        "        'g4dn.xlarge': 0.526,   # 1x T4 GPU\n",
        "        'g4dn.2xlarge': 0.752,  # 1x T4 GPU, more CPU\n",
        "        'g5.xlarge': 1.006,     # 1x A10G GPU\n",
        "        'p3.2xlarge': 3.06,     # 1x V100 GPU\n",
        "    }\n",
        "    \n",
        "    hourly_rate = instance_prices.get(instance_type, 0.526)\n",
        "    if use_spot:\n",
        "        hourly_rate *= 0.35  # 65% discount\n",
        "        label = f\"{instance_type} (Spot)\"\n",
        "    else:\n",
        "        label = f\"{instance_type} (On-Demand)\"\n",
        "    \n",
        "    total_hours = epochs * hours_per_epoch\n",
        "    compute_cost = total_hours * hourly_rate\n",
        "    storage_cost = model_params_millions * 0.004 * 0.023 * (total_hours / 720)\n",
        "    total = compute_cost + storage_cost\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"AWS COST ESTIMATE: {model_params_millions}M params, {epochs} epochs\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Instance: {label}\")\n",
        "    print(f\"Hours: {total_hours:.1f} ({hourly_rate:.3f}/hr)\")\n",
        "    print(f\"Compute: ${compute_cost:.2f}\")\n",
        "    print(f\"Storage (S3): ${storage_cost:.2f}\")\n",
        "    print(f\"\\nüí∞ TOTAL: ${total:.2f}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if use_spot:\n",
        "        on_demand = total_hours * instance_prices.get(instance_type, 0.526)\n",
        "        print(f\"‚úÖ Savings vs On-Demand: ${on_demand-compute_cost:.2f} ({((on_demand-compute_cost)/on_demand)*100:.0f}%)\")\n",
        "    \n",
        "    return total\n",
        "\n",
        "# Example: GPT-2 Small (recommended)\n",
        "print(\"Example 1: GPT-2 Small (124M) - Recommended\\n\")\n",
        "cost1 = estimate_aws_cost(124, epochs=10, hours_per_epoch=2, use_spot=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Example 2: GPT-2 Medium (355M) - Advanced\\n\")\n",
        "cost2 = estimate_aws_cost(355, epochs=5, hours_per_epoch=4, \n",
        "                          instance_type='g5.xlarge', use_spot=True)\n",
        "\n",
        "print(\"\\nüí° Recommendation: Start with GPT-2 Small. Total cost < $50!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Project Roadmap\n",
        "\n",
        "### What We'll Build:\n",
        "Complete LLM training and finetuning pipeline!\n",
        "\n",
        "**Chapter 2: Text Data Processing**\n",
        "- Tokenization (BPE)\n",
        "- Creating embeddings\n",
        "- Data loading and batching\n",
        "- AWS: S3 for datasets\n",
        "\n",
        "**Chapter 3: Attention Mechanisms**\n",
        "- Self-attention from scratch\n",
        "- Multi-head attention\n",
        "- Causal masking\n",
        "- AWS: SageMaker notebooks\n",
        "\n",
        "**Chapter 4: GPT Model**\n",
        "- Complete GPT architecture\n",
        "- Layer normalization\n",
        "- Residual connections\n",
        "- Text generation\n",
        "- AWS: Model checkpoints in S3\n",
        "\n",
        "**Chapter 5: Pretraining**\n",
        "- Training loop implementation\n",
        "- Loss calculation\n",
        "- Model evaluation\n",
        "- AWS: SageMaker Training with spot instances\n",
        "\n",
        "**Chapter 6: Classification Finetuning**\n",
        "- Spam classification\n",
        "- Transfer learning\n",
        "- Evaluation metrics\n",
        "- AWS: SageMaker finetuning\n",
        "\n",
        "**Chapter 7: Instruction Finetuning**\n",
        "- Instruction-following\n",
        "- Dataset preparation\n",
        "- LoRA (parameter-efficient)\n",
        "- Model deployment\n",
        "- AWS: SageMaker Inference Endpoint\n",
        "\n",
        "### Features:\n",
        "‚úÖ Complete from-scratch implementation  \n",
        "‚úÖ AWS-ready with cost optimization  \n",
        "‚úÖ Google Colab compatible  \n",
        "‚úÖ Production-ready code  \n",
        "‚úÖ Comprehensive documentation  \n",
        "‚úÖ Portfolio-ready presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Visualize project roadmap and chapter complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project roadmap visualization\n",
        "chapters = ['Ch 1:\\nFundamentals', 'Ch 2:\\nText Data', 'Ch 3:\\nAttention', \n",
        "            'Ch 4:\\nGPT Model', 'Ch 5:\\nPretraining', 'Ch 6:\\nClassification', \n",
        "            'Ch 7:\\nInstructions']\n",
        "complexity = [1, 3, 5, 7, 8, 6, 7]\n",
        "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#e67e22', '#f39c12', '#1abc9c']\n",
        "icons = ['üìö', 'üìù', 'üß†', 'ü§ñ', 'üöÄ', 'üéØ', 'üí¨']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Complexity chart\n",
        "bars = ax1.barh(chapters, complexity, color=colors, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_xlabel('Complexity Score', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Chapter Complexity', fontsize=13, fontweight='bold')\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "for bar, value in zip(bars, complexity):\n",
        "    ax1.text(value+0.2, bar.get_y()+bar.get_height()/2, \n",
        "             f'{value}/10', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Timeline\n",
        "ax2.axis('off')\n",
        "y_positions = np.linspace(0.9, 0.1, len(chapters))\n",
        "\n",
        "for i, (chapter, y_pos, color, icon) in enumerate(zip(chapters, y_positions, colors, icons)):\n",
        "    circle = plt.Circle((0.15, y_pos), 0.03, color=color, ec='black', linewidth=2)\n",
        "    ax2.add_patch(circle)\n",
        "    \n",
        "    if i < len(chapters) - 1:\n",
        "        ax2.plot([0.15, 0.15], [y_pos-0.03, y_positions[i+1]+0.03], \n",
        "                'k-', linewidth=2, alpha=0.5)\n",
        "    \n",
        "    ax2.text(0.22, y_pos, chapter, va='center', fontsize=11, fontweight='bold')\n",
        "    ax2.text(0.08, y_pos, icon, va='center', ha='center', fontsize=16)\n",
        "\n",
        "ax2.set_xlim(0, 1)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_title('Project Roadmap', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üéì You're in Chapter 1 - Fundamentals!\")\n",
        "print(\"   Ready to build? Let's move to Chapter 2! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Prerequisites Check\n",
        "\n",
        "### Required Knowledge:\n",
        "**Essential:**\n",
        "- Python: functions, classes, loops\n",
        "- NumPy: arrays, broadcasting\n",
        "- Basic ML: neural networks, gradient descent\n",
        "\n",
        "**Helpful:**\n",
        "- PyTorch: tensors, autograd, nn.Module\n",
        "- AWS: S3, SageMaker basics\n",
        "- Deep Learning: CNNs, RNNs concepts\n",
        "\n",
        "### Resources:\n",
        "- [PyTorch in 60 Minutes](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- [AWS SageMaker Guide](https://docs.aws.amazon.com/sagemaker/)\n",
        "- [Deep Learning Book](https://www.deeplearningbook.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell Purpose: Quick self-assessment of PyTorch and NumPy knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick knowledge check\n",
        "print(\"üß™ QUICK KNOWLEDGE CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test 1: Matrix multiplication\n",
        "print(\"\\n1. PyTorch Tensor Operations:\")\n",
        "try:\n",
        "    x = torch.randn(3, 4)\n",
        "    y = torch.randn(4, 5)\n",
        "    z = torch.matmul(x, y)\n",
        "    print(f\"   ‚úÖ Matmul: ({x.shape}) √ó ({y.shape}) = ({z.shape})\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "# Test 2: Broadcasting\n",
        "print(\"\\n2. Broadcasting:\")\n",
        "try:\n",
        "    a = torch.randn(3, 1)\n",
        "    b = torch.randn(1, 4)\n",
        "    c = a + b\n",
        "    print(f\"   ‚úÖ Broadcasting: ({a.shape}) + ({b.shape}) = ({c.shape})\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "# Test 3: Gradients\n",
        "print(\"\\n3. Automatic Differentiation:\")\n",
        "try:\n",
        "    x = torch.tensor([2.0], requires_grad=True)\n",
        "    y = x**2 + 3*x + 1\n",
        "    y.backward()\n",
        "    print(f\"   ‚úÖ Gradient of y=x¬≤+3x+1 at x=2: dy/dx = {x.grad.item():.1f}\")\n",
        "    print(f\"      (Expected: 7.0)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "# Test 4: GPU\n",
        "print(\"\\n4. GPU Availability:\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  No GPU (CPU mode - slower but works!)\")\n",
        "\n",
        "print(\"\\n\"+\"=\"*60)\n",
        "print(\"‚úÖ All checks passed! Ready to proceed.\")\n",
        "print(\"üí° If CPU-only, consider Google Colab or AWS SageMaker for faster training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Chapter Summary\n",
        "\n",
        "### What We Learned:\n",
        "1. ‚úÖ **LLM Basics**: Transformer architecture and components\n",
        "2. ‚úÖ **Training Pipeline**: Three-stage process\n",
        "3. ‚úÖ **Cost Optimization**: AWS strategies (< $100 total)\n",
        "4. ‚úÖ **Project Roadmap**: Clear path through 7 chapters\n",
        "5. ‚úÖ **Environment Setup**: Ready for AWS, Colab, or local\n",
        "\n",
        "### Key Takeaways:\n",
        "- LLMs can be built cost-effectively at small scale\n",
        "- Transformer architecture is the foundation\n",
        "- AWS offers flexible ML infrastructure\n",
        "- Hands-on production ML workflow experience\n",
        "\n",
        "### Next Steps:\n",
        "‚û°Ô∏è **Chapter 2**: Text data processing, tokenization, and embeddings!\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Resources\n",
        "\n",
        "**Papers:**\n",
        "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)\n",
        "- [BERT Paper](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "**AWS Documentation:**\n",
        "- [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/)\n",
        "- [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
        "- [Spot Instance Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)\n",
        "\n",
        "**Learning:**\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [Hugging Face Course](https://huggingface.co/course)\n",
        "\n",
        "**Ready for Chapter 2? Let's start building! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
